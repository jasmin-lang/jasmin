param int crypto_scalarmult_curve25519_amd64_64_121666 = 121666;
param int crypto_scalarmult_curve25519_amd64_64_38 = 38;
export fn crypto_scalarmult_curve25519_amd64_64_ladderstep(reg u64 workp){
	 reg u64 addt0;
	 reg u64 addt1;
	 reg bool cf;
	 reg u64 mul121666r4;
	 reg u64 mul121666rax;
	 reg u64 mul121666rdx;
	 reg u64 mul121666t1;
	 reg u64 mul121666t2;
	 reg u64 mul121666t3;
	 reg u64 mulc;
	 reg u64 mulr4;
	 reg u64 mulr5;
	 reg u64 mulr6;
	 reg u64 mulr7;
	 reg u64 mulr8;
	 reg u64 mulrax;
	 reg u64 mulrdx;
	 reg u64 mulx0;
	 reg u64 mulx1;
	 reg u64 mulx2;
	 reg u64 mulx3;
	 reg u64 mulzero;
	 reg u64 squarer4;
	 reg u64 squarer5;
	 reg u64 squarer6;
	 reg u64 squarer7;
	 reg u64 squarer8;
	 reg u64 squarerax;
	 reg u64 squarerdx;
	 reg u64 squaret1;
	 reg u64 squaret2;
	 reg u64 squaret3;
	 reg u64 squarezero;
	 reg u64 subt0;
	 reg u64 subt1;
	 reg u64 t10;
	 stack u64 t10_stack;
	 reg u64 t11;
	 stack u64 t11_stack;
	 reg u64 t12;
	 stack u64 t12_stack;
	 reg u64 t13;
	 stack u64 t13_stack;
	 reg u64 t20;
	 stack u64 t20_stack;
	 reg u64 t21;
	 stack u64 t21_stack;
	 reg u64 t22;
	 stack u64 t22_stack;
	 reg u64 t23;
	 stack u64 t23_stack;
	 reg u64 t30;
	 stack u64 t30_stack;
	 reg u64 t31;
	 stack u64 t31_stack;
	 reg u64 t32;
	 stack u64 t32_stack;
	 reg u64 t33;
	 stack u64 t33_stack;
	 reg u64 t40;
	 stack u64 t40_stack;
	 reg u64 t41;
	 stack u64 t41_stack;
	 reg u64 t42;
	 stack u64 t42_stack;
	 reg u64 t43;
	 stack u64 t43_stack;
	 reg u64 t50;
	 stack u64 t50_stack;
	 reg u64 t51;
	 stack u64 t51_stack;
	 reg u64 t52;
	 stack u64 t52_stack;
	 reg u64 t53;
	 stack u64 t53_stack;
	 reg u64 t60;
	 stack u64 t60_stack;
	 reg u64 t61;
	 stack u64 t61_stack;
	 reg u64 t62;
	 stack u64 t62_stack;
	 reg u64 t63;
	 stack u64 t63_stack;
	 reg u64 t70;
	 stack u64 t70_stack;
	 reg u64 t71;
	 stack u64 t71_stack;
	 reg u64 t72;
	 stack u64 t72_stack;
	 reg u64 t73;
	 stack u64 t73_stack;
	 reg u64 t80;
	 reg u64 t81;
	 reg u64 t82;
	 reg u64 t83;
	 reg u64 t90;
	 stack u64 t90_stack;
	 reg u64 t91;
	 stack u64 t91_stack;
	 reg u64 t92;
	 stack u64 t92_stack;
	 reg u64 t93;
	 stack u64 t93_stack;
	 reg u64 xp0;
	 reg u64 xp1;
	 reg u64 xp2;
	 reg u64 xp3;
	 reg u64 xq0;
	 reg u64 xq1;
	 reg u64 xq2;
	 reg u64 xq3;
	 reg u64 zp0;
	 reg u64 zp1;
	 reg u64 zp2;
	 reg u64 zp3;
	 reg u64 zq0;
	 reg u64 zq1;
	 reg u64 zq2;
	 reg u64 zq3;

	t10 = [workp + 4*8];
	t11 = [workp + 5*8];
	t12 = [workp + 6*8];
	t13 = [workp + 7*8];
	t20 = t10;
	t21 = t11;
	t22 = t12;
	t23 = t13;
	cf, t10 += [workp + 8*8];
	cf, t11 += [workp + 9*8] + cf;
	cf, t12 += [workp + 10*8] + cf;
	cf, t13 += [workp + 11*8] + cf;
	addt0 = #x86_MOV(0);
	addt1 = 38;
	addt1 = addt0 if !cf;
	cf, t10 += addt1;
	cf, t11 += addt0 + cf;
	cf, t12 += addt0 + cf;
	cf, t13 += addt0 + cf;
	addt0 = addt1 if cf;
	t10 += addt0;
	cf, t20 -= [workp + 8*8];
	cf, t21 -= [workp + 9*8] - cf;
	cf, t22 -= [workp + 10*8] - cf;
	cf, t23 -= [workp + 11*8] - cf;
	subt0 = #x86_MOV(0);
	subt1 = 38;
	subt1 = subt0 if !cf;
	cf, t20 -= subt1;
	cf, t21 -= subt0 - cf;
	cf, t22 -= subt0 - cf;
	cf, t23 -= subt0 - cf;
	subt0 = subt1 if cf;
	t20 -= subt0;
	t10_stack = t10;
	t11_stack = t11;
	t12_stack = t12;
	t13_stack = t13;
	t20_stack = t20;
	t21_stack = t21;
	t22_stack = t22;
	t23_stack = t23;
	squarer7 = #x86_MOV(0);
	squarerax = t21_stack;
	squarerdx, squarerax = squarerax * t20_stack;
	t71 = squarerax;
	t72 = squarerdx;
	squarerax = t22_stack;
	squarerdx, squarerax = squarerax * t21_stack;
	t73 = squarerax;
	squarer4 = squarerdx;
	squarerax = t23_stack;
	squarerdx, squarerax = squarerax * t22_stack;
	squarer5 = squarerax;
	squarer6 = squarerdx;
	squarerax = t22_stack;
	squarerdx, squarerax = squarerax * t20_stack;
	cf, t72 += squarerax;
	cf, t73 += squarerdx + cf;
	_, squarer4 += 0 + cf;
	squarerax = t23_stack;
	squarerdx, squarerax = squarerax * t21_stack;
	cf, squarer4 += squarerax;
	cf, squarer5 += squarerdx + cf;
	_, squarer6 += 0 + cf;
	squarerax = t23_stack;
	squarerdx, squarerax = squarerax * t20_stack;
	cf, t73 += squarerax;
	cf, squarer4 += squarerdx + cf;
	cf, squarer5 += 0 + cf;
	cf, squarer6 += 0 + cf;
	_, squarer7 += 0 + cf;
	cf, t71 += t71;
	cf, t72 += t72 + cf;
	cf, t73 += t73 + cf;
	cf, squarer4 += squarer4 + cf;
	cf, squarer5 += squarer5 + cf;
	cf, squarer6 += squarer6 + cf;
	_, squarer7 += squarer7 + cf;
	squarerax = t20_stack;
	squarerdx, squarerax = squarerax * t20_stack;
	t70 = squarerax;
	squaret1 = squarerdx;
	squarerax = t21_stack;
	squarerdx, squarerax = squarerax * t21_stack;
	squaret2 = squarerax;
	squaret3 = squarerdx;
	squarerax = t22_stack;
	squarerdx, squarerax = squarerax * t22_stack;
	cf, t71 += squaret1;
	cf, t72 += squaret2 + cf;
	cf, t73 += squaret3 + cf;
	cf, squarer4 += squarerax + cf;
	cf, squarer5 += squarerdx + cf;
	cf, squarer6 += 0 + cf;
	_, squarer7 += 0 + cf;
	squarerax = t23_stack;
	squarerdx, squarerax = squarerax * t23_stack;
	cf, squarer6 += squarerax;
	_, squarer7 += squarerdx + cf;
	squarerax = squarer4;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	squarer4 = squarerax;
	squarerax = squarer5;
	squarer5 = squarerdx;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, squarer5 += squarerax;
	squarerax = squarer6;
	squarer6 = #x86_MOV(0);
	_, squarer6 += squarerdx + cf;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, squarer6 += squarerax;
	squarerax = squarer7;
	squarer7 = #x86_MOV(0);
	_, squarer7 += squarerdx + cf;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, squarer7 += squarerax;
	squarer8 = #x86_MOV(0);
	_, squarer8 += squarerdx + cf;
	cf, t70 += squarer4;
	cf, t71 += squarer5 + cf;
	cf, t72 += squarer6 + cf;
	cf, t73 += squarer7 + cf;
	squarezero = #x86_MOV(0);
	_, squarer8 += squarezero + cf;
	squarer8 *= 38;
	cf, t70 += squarer8;
	cf, t71 += squarezero + cf;
	cf, t72 += squarezero + cf;
	cf, t73 += squarezero + cf;
	_, squarezero += squarezero + cf;
	squarezero *= 38;
	t70 += squarezero;
	t70_stack = t70;
	t71_stack = t71;
	t72_stack = t72;
	t73_stack = t73;
	squarer7 = #x86_MOV(0);
	squarerax = t11_stack;
	squarerdx, squarerax = squarerax * t10_stack;
	t61 = squarerax;
	t62 = squarerdx;
	squarerax = t12_stack;
	squarerdx, squarerax = squarerax * t11_stack;
	t63 = squarerax;
	squarer4 = squarerdx;
	squarerax = t13_stack;
	squarerdx, squarerax = squarerax * t12_stack;
	squarer5 = squarerax;
	squarer6 = squarerdx;
	squarerax = t12_stack;
	squarerdx, squarerax = squarerax * t10_stack;
	cf, t62 += squarerax;
	cf, t63 += squarerdx + cf;
	_, squarer4 += 0 + cf;
	squarerax = t13_stack;
	squarerdx, squarerax = squarerax * t11_stack;
	cf, squarer4 += squarerax;
	cf, squarer5 += squarerdx + cf;
	_, squarer6 += 0 + cf;
	squarerax = t13_stack;
	squarerdx, squarerax = squarerax * t10_stack;
	cf, t63 += squarerax;
	cf, squarer4 += squarerdx + cf;
	cf, squarer5 += 0 + cf;
	cf, squarer6 += 0 + cf;
	_, squarer7 += 0 + cf;
	cf, t61 += t61;
	cf, t62 += t62 + cf;
	cf, t63 += t63 + cf;
	cf, squarer4 += squarer4 + cf;
	cf, squarer5 += squarer5 + cf;
	cf, squarer6 += squarer6 + cf;
	_, squarer7 += squarer7 + cf;
	squarerax = t10_stack;
	squarerdx, squarerax = squarerax * t10_stack;
	t60 = squarerax;
	squaret1 = squarerdx;
	squarerax = t11_stack;
	squarerdx, squarerax = squarerax * t11_stack;
	squaret2 = squarerax;
	squaret3 = squarerdx;
	squarerax = t12_stack;
	squarerdx, squarerax = squarerax * t12_stack;
	cf, t61 += squaret1;
	cf, t62 += squaret2 + cf;
	cf, t63 += squaret3 + cf;
	cf, squarer4 += squarerax + cf;
	cf, squarer5 += squarerdx + cf;
	cf, squarer6 += 0 + cf;
	_, squarer7 += 0 + cf;
	squarerax = t13_stack;
	squarerdx, squarerax = squarerax * t13_stack;
	cf, squarer6 += squarerax;
	_, squarer7 += squarerdx + cf;
	squarerax = squarer4;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	squarer4 = squarerax;
	squarerax = squarer5;
	squarer5 = squarerdx;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, squarer5 += squarerax;
	squarerax = squarer6;
	squarer6 = #x86_MOV(0);
	_, squarer6 += squarerdx + cf;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, squarer6 += squarerax;
	squarerax = squarer7;
	squarer7 = #x86_MOV(0);
	_, squarer7 += squarerdx + cf;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, squarer7 += squarerax;
	squarer8 = #x86_MOV(0);
	_, squarer8 += squarerdx + cf;
	cf, t60 += squarer4;
	cf, t61 += squarer5 + cf;
	cf, t62 += squarer6 + cf;
	cf, t63 += squarer7 + cf;
	squarezero = #x86_MOV(0);
	_, squarer8 += squarezero + cf;
	squarer8 *= 38;
	cf, t60 += squarer8;
	cf, t61 += squarezero + cf;
	cf, t62 += squarezero + cf;
	cf, t63 += squarezero + cf;
	_, squarezero += squarezero + cf;
	squarezero *= 38;
	t60 += squarezero;
	t60_stack = t60;
	t61_stack = t61;
	t62_stack = t62;
	t63_stack = t63;
	t50 = t60;
	t51 = t61;
	t52 = t62;
	t53 = t63;
	cf, t50 -= t70_stack;
	cf, t51 -= t71_stack - cf;
	cf, t52 -= t72_stack - cf;
	cf, t53 -= t73_stack - cf;
	subt0 = #x86_MOV(0);
	subt1 = 38;
	subt1 = subt0 if !cf;
	cf, t50 -= subt1;
	cf, t51 -= subt0 - cf;
	cf, t52 -= subt0 - cf;
	cf, t53 -= subt0 - cf;
	subt0 = subt1 if cf;
	t50 -= subt0;
	t50_stack = t50;
	t51_stack = t51;
	t52_stack = t52;
	t53_stack = t53;
	t30 = [workp + 12*8];
	t31 = [workp + 13*8];
	t32 = [workp + 14*8];
	t33 = [workp + 15*8];
	t40 = t30;
	t41 = t31;
	t42 = t32;
	t43 = t33;
	cf, t30 += [workp + 16*8];
	cf, t31 += [workp + 17*8] + cf;
	cf, t32 += [workp + 18*8] + cf;
	cf, t33 += [workp + 19*8] + cf;
	addt0 = #x86_MOV(0);
	addt1 = 38;
	addt1 = addt0 if !cf;
	cf, t30 += addt1;
	cf, t31 += addt0 + cf;
	cf, t32 += addt0 + cf;
	cf, t33 += addt0 + cf;
	addt0 = addt1 if cf;
	t30 += addt0;
	cf, t40 -= [workp + 16*8];
	cf, t41 -= [workp + 17*8] - cf;
	cf, t42 -= [workp + 18*8] - cf;
	cf, t43 -= [workp + 19*8] - cf;
	subt0 = #x86_MOV(0);
	subt1 = 38;
	subt1 = subt0 if !cf;
	cf, t40 -= subt1;
	cf, t41 -= subt0 - cf;
	cf, t42 -= subt0 - cf;
	cf, t43 -= subt0 - cf;
	subt0 = subt1 if cf;
	t40 -= subt0;
	t30_stack = t30;
	t31_stack = t31;
	t32_stack = t32;
	t33_stack = t33;
	t40_stack = t40;
	t41_stack = t41;
	t42_stack = t42;
	t43_stack = t43;
	mulr4 = #x86_MOV(0);
	mulr5 = #x86_MOV(0);
	mulr6 = #x86_MOV(0);
	mulr7 = #x86_MOV(0);
	mulx0 = t30_stack;
	mulrax = t20_stack;
	mulrdx, mulrax = mulrax * mulx0;
	t90 = mulrax;
	t91 = mulrdx;
	mulrax = t21_stack;
	mulrdx, mulrax = mulrax * mulx0;
	cf, t91 += mulrax;
	t92 = #x86_MOV(0);
	_, t92 += mulrdx + cf;
	mulrax = t22_stack;
	mulrdx, mulrax = mulrax * mulx0;
	cf, t92 += mulrax;
	t93 = #x86_MOV(0);
	_, t93 += mulrdx + cf;
	mulrax = t23_stack;
	mulrdx, mulrax = mulrax * mulx0;
	cf, t93 += mulrax;
	_, mulr4 += mulrdx + cf;
	mulx1 = t31_stack;
	mulrax = t20_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, t91 += mulrax;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t21_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, t92 += mulrax;
	_, mulrdx += 0 + cf;
	cf, t92 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t22_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, t93 += mulrax;
	_, mulrdx += 0 + cf;
	cf, t93 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t23_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, mulr4 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr4 += mulc;
	_, mulr5 += mulrdx + cf;
	mulx2 = t32_stack;
	mulrax = t20_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, t92 += mulrax;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t21_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, t93 += mulrax;
	_, mulrdx += 0 + cf;
	cf, t93 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t22_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, mulr4 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr4 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t23_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, mulr5 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr5 += mulc;
	_, mulr6 += mulrdx + cf;
	mulx3 = t33_stack;
	mulrax = t20_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, t93 += mulrax;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t21_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, mulr4 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr4 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t22_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, mulr5 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr5 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t23_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, mulr6 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr6 += mulc;
	_, mulr7 += mulrdx + cf;
	mulrax = mulr4;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	mulr4 = mulrax;
	mulrax = mulr5;
	mulr5 = mulrdx;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, mulr5 += mulrax;
	mulrax = mulr6;
	mulr6 = #x86_MOV(0);
	_, mulr6 += mulrdx + cf;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, mulr6 += mulrax;
	mulrax = mulr7;
	mulr7 = #x86_MOV(0);
	_, mulr7 += mulrdx + cf;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, mulr7 += mulrax;
	mulr8 = #x86_MOV(0);
	_, mulr8 += mulrdx + cf;
	cf, t90 += mulr4;
	cf, t91 += mulr5 + cf;
	cf, t92 += mulr6 + cf;
	cf, t93 += mulr7 + cf;
	mulzero = #x86_MOV(0);
	_, mulr8 += mulzero + cf;
	mulr8 *= 38;
	cf, t90 += mulr8;
	cf, t91 += mulzero + cf;
	cf, t92 += mulzero + cf;
	cf, t93 += mulzero + cf;
	_, mulzero += mulzero + cf;
	mulzero *= 38;
	t90 += mulzero;
	t90_stack = t90;
	t91_stack = t91;
	t92_stack = t92;
	t93_stack = t93;
	mulr4 = #x86_MOV(0);
	mulr5 = #x86_MOV(0);
	mulr6 = #x86_MOV(0);
	mulr7 = #x86_MOV(0);
	mulx0 = t40_stack;
	mulrax = t10_stack;
	mulrdx, mulrax = mulrax * mulx0;
	t80 = mulrax;
	t81 = mulrdx;
	mulrax = t11_stack;
	mulrdx, mulrax = mulrax * mulx0;
	cf, t81 += mulrax;
	t82 = #x86_MOV(0);
	_, t82 += mulrdx + cf;
	mulrax = t12_stack;
	mulrdx, mulrax = mulrax * mulx0;
	cf, t82 += mulrax;
	t83 = #x86_MOV(0);
	_, t83 += mulrdx + cf;
	mulrax = t13_stack;
	mulrdx, mulrax = mulrax * mulx0;
	cf, t83 += mulrax;
	_, mulr4 += mulrdx + cf;
	mulx1 = t41_stack;
	mulrax = t10_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, t81 += mulrax;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t11_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, t82 += mulrax;
	_, mulrdx += 0 + cf;
	cf, t82 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t12_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, t83 += mulrax;
	_, mulrdx += 0 + cf;
	cf, t83 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t13_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, mulr4 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr4 += mulc;
	_, mulr5 += mulrdx + cf;
	mulx2 = t42_stack;
	mulrax = t10_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, t82 += mulrax;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t11_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, t83 += mulrax;
	_, mulrdx += 0 + cf;
	cf, t83 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t12_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, mulr4 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr4 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t13_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, mulr5 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr5 += mulc;
	_, mulr6 += mulrdx + cf;
	mulx3 = t43_stack;
	mulrax = t10_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, t83 += mulrax;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t11_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, mulr4 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr4 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t12_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, mulr5 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr5 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t13_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, mulr6 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr6 += mulc;
	_, mulr7 += mulrdx + cf;
	mulrax = mulr4;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	mulr4 = mulrax;
	mulrax = mulr5;
	mulr5 = mulrdx;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, mulr5 += mulrax;
	mulrax = mulr6;
	mulr6 = #x86_MOV(0);
	_, mulr6 += mulrdx + cf;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, mulr6 += mulrax;
	mulrax = mulr7;
	mulr7 = #x86_MOV(0);
	_, mulr7 += mulrdx + cf;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, mulr7 += mulrax;
	mulr8 = #x86_MOV(0);
	_, mulr8 += mulrdx + cf;
	cf, t80 += mulr4;
	cf, t81 += mulr5 + cf;
	cf, t82 += mulr6 + cf;
	cf, t83 += mulr7 + cf;
	mulzero = #x86_MOV(0);
	_, mulr8 += mulzero + cf;
	mulr8 *= 38;
	cf, t80 += mulr8;
	cf, t81 += mulzero + cf;
	cf, t82 += mulzero + cf;
	cf, t83 += mulzero + cf;
	_, mulzero += mulzero + cf;
	mulzero *= 38;
	t80 += mulzero;
	zq0 = t80;
	zq1 = t81;
	zq2 = t82;
	zq3 = t83;
	cf, zq0 -= t90_stack;
	cf, zq1 -= t91_stack - cf;
	cf, zq2 -= t92_stack - cf;
	cf, zq3 -= t93_stack - cf;
	subt0 = #x86_MOV(0);
	subt1 = 38;
	subt1 = subt0 if !cf;
	cf, zq0 -= subt1;
	cf, zq1 -= subt0 - cf;
	cf, zq2 -= subt0 - cf;
	cf, zq3 -= subt0 - cf;
	subt0 = subt1 if cf;
	zq0 -= subt0;
	cf, t80 += t90_stack;
	cf, t81 += t91_stack + cf;
	cf, t82 += t92_stack + cf;
	cf, t83 += t93_stack + cf;
	addt0 = #x86_MOV(0);
	addt1 = 38;
	addt1 = addt0 if !cf;
	cf, t80 += addt1;
	cf, t81 += addt0 + cf;
	cf, t82 += addt0 + cf;
	cf, t83 += addt0 + cf;
	addt0 = addt1 if cf;
	t80 += addt0;
	[workp + 12*8] = t80;
	[workp + 13*8] = t81;
	[workp + 14*8] = t82;
	[workp + 15*8] = t83;
	[workp + 16*8] = zq0;
	[workp + 17*8] = zq1;
	[workp + 18*8] = zq2;
	[workp + 19*8] = zq3;
	squarer7 = #x86_MOV(0);
	squarerax = [workp + 13*8];
	squarerdx, squarerax = squarerax * [workp + 12*8];
	xq1 = squarerax;
	xq2 = squarerdx;
	squarerax = [workp + 14*8];
	squarerdx, squarerax = squarerax * [workp + 13*8];
	xq3 = squarerax;
	squarer4 = squarerdx;
	squarerax = [workp + 15*8];
	squarerdx, squarerax = squarerax * [workp + 14*8];
	squarer5 = squarerax;
	squarer6 = squarerdx;
	squarerax = [workp + 14*8];
	squarerdx, squarerax = squarerax * [workp + 12*8];
	cf, xq2 += squarerax;
	cf, xq3 += squarerdx + cf;
	_, squarer4 += 0 + cf;
	squarerax = [workp + 15*8];
	squarerdx, squarerax = squarerax * [workp + 13*8];
	cf, squarer4 += squarerax;
	cf, squarer5 += squarerdx + cf;
	_, squarer6 += 0 + cf;
	squarerax = [workp + 15*8];
	squarerdx, squarerax = squarerax * [workp + 12*8];
	cf, xq3 += squarerax;
	cf, squarer4 += squarerdx + cf;
	cf, squarer5 += 0 + cf;
	cf, squarer6 += 0 + cf;
	_, squarer7 += 0 + cf;
	cf, xq1 += xq1;
	cf, xq2 += xq2 + cf;
	cf, xq3 += xq3 + cf;
	cf, squarer4 += squarer4 + cf;
	cf, squarer5 += squarer5 + cf;
	cf, squarer6 += squarer6 + cf;
	_, squarer7 += squarer7 + cf;
	squarerax = [workp + 12*8];
	squarerdx, squarerax = squarerax * [workp + 12*8];
	xq0 = squarerax;
	squaret1 = squarerdx;
	squarerax = [workp + 13*8];
	squarerdx, squarerax = squarerax * [workp + 13*8];
	squaret2 = squarerax;
	squaret3 = squarerdx;
	squarerax = [workp + 14*8];
	squarerdx, squarerax = squarerax * [workp + 14*8];
	cf, xq1 += squaret1;
	cf, xq2 += squaret2 + cf;
	cf, xq3 += squaret3 + cf;
	cf, squarer4 += squarerax + cf;
	cf, squarer5 += squarerdx + cf;
	cf, squarer6 += 0 + cf;
	_, squarer7 += 0 + cf;
	squarerax = [workp + 15*8];
	squarerdx, squarerax = squarerax * [workp + 15*8];
	cf, squarer6 += squarerax;
	_, squarer7 += squarerdx + cf;
	squarerax = squarer4;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	squarer4 = squarerax;
	squarerax = squarer5;
	squarer5 = squarerdx;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, squarer5 += squarerax;
	squarerax = squarer6;
	squarer6 = #x86_MOV(0);
	_, squarer6 += squarerdx + cf;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, squarer6 += squarerax;
	squarerax = squarer7;
	squarer7 = #x86_MOV(0);
	_, squarer7 += squarerdx + cf;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, squarer7 += squarerax;
	squarer8 = #x86_MOV(0);
	_, squarer8 += squarerdx + cf;
	cf, xq0 += squarer4;
	cf, xq1 += squarer5 + cf;
	cf, xq2 += squarer6 + cf;
	cf, xq3 += squarer7 + cf;
	squarezero = #x86_MOV(0);
	_, squarer8 += squarezero + cf;
	squarer8 *= 38;
	cf, xq0 += squarer8;
	cf, xq1 += squarezero + cf;
	cf, xq2 += squarezero + cf;
	cf, xq3 += squarezero + cf;
	_, squarezero += squarezero + cf;
	squarezero *= 38;
	xq0 += squarezero;
	[workp + 12*8] = xq0;
	[workp + 13*8] = xq1;
	[workp + 14*8] = xq2;
	[workp + 15*8] = xq3;
	squarer7 = #x86_MOV(0);
	squarerax = [workp + 17*8];
	squarerdx, squarerax = squarerax * [workp + 16*8];
	zq1 = squarerax;
	zq2 = squarerdx;
	squarerax = [workp + 18*8];
	squarerdx, squarerax = squarerax * [workp + 17*8];
	zq3 = squarerax;
	squarer4 = squarerdx;
	squarerax = [workp + 19*8];
	squarerdx, squarerax = squarerax * [workp + 18*8];
	squarer5 = squarerax;
	squarer6 = squarerdx;
	squarerax = [workp + 18*8];
	squarerdx, squarerax = squarerax * [workp + 16*8];
	cf, zq2 += squarerax;
	cf, zq3 += squarerdx + cf;
	_, squarer4 += 0 + cf;
	squarerax = [workp + 19*8];
	squarerdx, squarerax = squarerax * [workp + 17*8];
	cf, squarer4 += squarerax;
	cf, squarer5 += squarerdx + cf;
	_, squarer6 += 0 + cf;
	squarerax = [workp + 19*8];
	squarerdx, squarerax = squarerax * [workp + 16*8];
	cf, zq3 += squarerax;
	cf, squarer4 += squarerdx + cf;
	cf, squarer5 += 0 + cf;
	cf, squarer6 += 0 + cf;
	_, squarer7 += 0 + cf;
	cf, zq1 += zq1;
	cf, zq2 += zq2 + cf;
	cf, zq3 += zq3 + cf;
	cf, squarer4 += squarer4 + cf;
	cf, squarer5 += squarer5 + cf;
	cf, squarer6 += squarer6 + cf;
	_, squarer7 += squarer7 + cf;
	squarerax = [workp + 16*8];
	squarerdx, squarerax = squarerax * [workp + 16*8];
	zq0 = squarerax;
	squaret1 = squarerdx;
	squarerax = [workp + 17*8];
	squarerdx, squarerax = squarerax * [workp + 17*8];
	squaret2 = squarerax;
	squaret3 = squarerdx;
	squarerax = [workp + 18*8];
	squarerdx, squarerax = squarerax * [workp + 18*8];
	cf, zq1 += squaret1;
	cf, zq2 += squaret2 + cf;
	cf, zq3 += squaret3 + cf;
	cf, squarer4 += squarerax + cf;
	cf, squarer5 += squarerdx + cf;
	cf, squarer6 += 0 + cf;
	_, squarer7 += 0 + cf;
	squarerax = [workp + 19*8];
	squarerdx, squarerax = squarerax * [workp + 19*8];
	cf, squarer6 += squarerax;
	_, squarer7 += squarerdx + cf;
	squarerax = squarer4;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	squarer4 = squarerax;
	squarerax = squarer5;
	squarer5 = squarerdx;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, squarer5 += squarerax;
	squarerax = squarer6;
	squarer6 = #x86_MOV(0);
	_, squarer6 += squarerdx + cf;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, squarer6 += squarerax;
	squarerax = squarer7;
	squarer7 = #x86_MOV(0);
	_, squarer7 += squarerdx + cf;
	squarerdx, squarerax = squarerax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, squarer7 += squarerax;
	squarer8 = #x86_MOV(0);
	_, squarer8 += squarerdx + cf;
	cf, zq0 += squarer4;
	cf, zq1 += squarer5 + cf;
	cf, zq2 += squarer6 + cf;
	cf, zq3 += squarer7 + cf;
	squarezero = #x86_MOV(0);
	_, squarer8 += squarezero + cf;
	squarer8 *= 38;
	cf, zq0 += squarer8;
	cf, zq1 += squarezero + cf;
	cf, zq2 += squarezero + cf;
	cf, zq3 += squarezero + cf;
	_, squarezero += squarezero + cf;
	squarezero *= 38;
	zq0 += squarezero;
	[workp + 16*8] = zq0;
	[workp + 17*8] = zq1;
	[workp + 18*8] = zq2;
	[workp + 19*8] = zq3;
	mulr4 = #x86_MOV(0);
	mulr5 = #x86_MOV(0);
	mulr6 = #x86_MOV(0);
	mulr7 = #x86_MOV(0);
	mulx0 = [workp + 16*8];
	mulrax = [workp + 0*8];
	mulrdx, mulrax = mulrax * mulx0;
	zq0 = mulrax;
	zq1 = mulrdx;
	mulrax = [workp + 1*8];
	mulrdx, mulrax = mulrax * mulx0;
	cf, zq1 += mulrax;
	zq2 = #x86_MOV(0);
	_, zq2 += mulrdx + cf;
	mulrax = [workp + 2*8];
	mulrdx, mulrax = mulrax * mulx0;
	cf, zq2 += mulrax;
	zq3 = #x86_MOV(0);
	_, zq3 += mulrdx + cf;
	mulrax = [workp + 3*8];
	mulrdx, mulrax = mulrax * mulx0;
	cf, zq3 += mulrax;
	_, mulr4 += mulrdx + cf;
	mulx1 = [workp + 17*8];
	mulrax = [workp + 0*8];
	mulrdx, mulrax = mulrax * mulx1;
	cf, zq1 += mulrax;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = [workp + 1*8];
	mulrdx, mulrax = mulrax * mulx1;
	cf, zq2 += mulrax;
	_, mulrdx += 0 + cf;
	cf, zq2 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = [workp + 2*8];
	mulrdx, mulrax = mulrax * mulx1;
	cf, zq3 += mulrax;
	_, mulrdx += 0 + cf;
	cf, zq3 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = [workp + 3*8];
	mulrdx, mulrax = mulrax * mulx1;
	cf, mulr4 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr4 += mulc;
	_, mulr5 += mulrdx + cf;
	mulx2 = [workp + 18*8];
	mulrax = [workp + 0*8];
	mulrdx, mulrax = mulrax * mulx2;
	cf, zq2 += mulrax;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = [workp + 1*8];
	mulrdx, mulrax = mulrax * mulx2;
	cf, zq3 += mulrax;
	_, mulrdx += 0 + cf;
	cf, zq3 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = [workp + 2*8];
	mulrdx, mulrax = mulrax * mulx2;
	cf, mulr4 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr4 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = [workp + 3*8];
	mulrdx, mulrax = mulrax * mulx2;
	cf, mulr5 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr5 += mulc;
	_, mulr6 += mulrdx + cf;
	mulx3 = [workp + 19*8];
	mulrax = [workp + 0*8];
	mulrdx, mulrax = mulrax * mulx3;
	cf, zq3 += mulrax;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = [workp + 1*8];
	mulrdx, mulrax = mulrax * mulx3;
	cf, mulr4 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr4 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = [workp + 2*8];
	mulrdx, mulrax = mulrax * mulx3;
	cf, mulr5 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr5 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = [workp + 3*8];
	mulrdx, mulrax = mulrax * mulx3;
	cf, mulr6 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr6 += mulc;
	_, mulr7 += mulrdx + cf;
	mulrax = mulr4;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	mulr4 = mulrax;
	mulrax = mulr5;
	mulr5 = mulrdx;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, mulr5 += mulrax;
	mulrax = mulr6;
	mulr6 = #x86_MOV(0);
	_, mulr6 += mulrdx + cf;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, mulr6 += mulrax;
	mulrax = mulr7;
	mulr7 = #x86_MOV(0);
	_, mulr7 += mulrdx + cf;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, mulr7 += mulrax;
	mulr8 = #x86_MOV(0);
	_, mulr8 += mulrdx + cf;
	cf, zq0 += mulr4;
	cf, zq1 += mulr5 + cf;
	cf, zq2 += mulr6 + cf;
	cf, zq3 += mulr7 + cf;
	mulzero = #x86_MOV(0);
	_, mulr8 += mulzero + cf;
	mulr8 *= 38;
	cf, zq0 += mulr8;
	cf, zq1 += mulzero + cf;
	cf, zq2 += mulzero + cf;
	cf, zq3 += mulzero + cf;
	_, mulzero += mulzero + cf;
	mulzero *= 38;
	zq0 += mulzero;
	[workp + 16*8] = zq0;
	[workp + 17*8] = zq1;
	[workp + 18*8] = zq2;
	[workp + 19*8] = zq3;
	mulr4 = #x86_MOV(0);
	mulr5 = #x86_MOV(0);
	mulr6 = #x86_MOV(0);
	mulr7 = #x86_MOV(0);
	mulx0 = t60_stack;
	mulrax = t70_stack;
	mulrdx, mulrax = mulrax * mulx0;
	xp0 = mulrax;
	xp1 = mulrdx;
	mulrax = t71_stack;
	mulrdx, mulrax = mulrax * mulx0;
	cf, xp1 += mulrax;
	xp2 = #x86_MOV(0);
	_, xp2 += mulrdx + cf;
	mulrax = t72_stack;
	mulrdx, mulrax = mulrax * mulx0;
	cf, xp2 += mulrax;
	xp3 = #x86_MOV(0);
	_, xp3 += mulrdx + cf;
	mulrax = t73_stack;
	mulrdx, mulrax = mulrax * mulx0;
	cf, xp3 += mulrax;
	_, mulr4 += mulrdx + cf;
	mulx1 = t61_stack;
	mulrax = t70_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, xp1 += mulrax;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t71_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, xp2 += mulrax;
	_, mulrdx += 0 + cf;
	cf, xp2 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t72_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, xp3 += mulrax;
	_, mulrdx += 0 + cf;
	cf, xp3 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t73_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, mulr4 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr4 += mulc;
	_, mulr5 += mulrdx + cf;
	mulx2 = t62_stack;
	mulrax = t70_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, xp2 += mulrax;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t71_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, xp3 += mulrax;
	_, mulrdx += 0 + cf;
	cf, xp3 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t72_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, mulr4 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr4 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t73_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, mulr5 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr5 += mulc;
	_, mulr6 += mulrdx + cf;
	mulx3 = t63_stack;
	mulrax = t70_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, xp3 += mulrax;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t71_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, mulr4 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr4 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t72_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, mulr5 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr5 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t73_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, mulr6 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr6 += mulc;
	_, mulr7 += mulrdx + cf;
	mulrax = mulr4;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	mulr4 = mulrax;
	mulrax = mulr5;
	mulr5 = mulrdx;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, mulr5 += mulrax;
	mulrax = mulr6;
	mulr6 = #x86_MOV(0);
	_, mulr6 += mulrdx + cf;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, mulr6 += mulrax;
	mulrax = mulr7;
	mulr7 = #x86_MOV(0);
	_, mulr7 += mulrdx + cf;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, mulr7 += mulrax;
	mulr8 = #x86_MOV(0);
	_, mulr8 += mulrdx + cf;
	cf, xp0 += mulr4;
	cf, xp1 += mulr5 + cf;
	cf, xp2 += mulr6 + cf;
	cf, xp3 += mulr7 + cf;
	mulzero = #x86_MOV(0);
	_, mulr8 += mulzero + cf;
	mulr8 *= 38;
	cf, xp0 += mulr8;
	cf, xp1 += mulzero + cf;
	cf, xp2 += mulzero + cf;
	cf, xp3 += mulzero + cf;
	_, mulzero += mulzero + cf;
	mulzero *= 38;
	xp0 += mulzero;
	[workp + 4*8] = xp0;
	[workp + 5*8] = xp1;
	[workp + 6*8] = xp2;
	[workp + 7*8] = xp3;
	mul121666rax = t50_stack;
	mul121666rdx, mul121666rax = mul121666rax * crypto_scalarmult_curve25519_amd64_64_121666;
	zp0 = mul121666rax;
	zp1 = mul121666rdx;
	mul121666rax = t52_stack;
	mul121666rdx, mul121666rax = mul121666rax * crypto_scalarmult_curve25519_amd64_64_121666;
	zp2 = mul121666rax;
	zp3 = mul121666rdx;
	mul121666rax = t51_stack;
	mul121666rdx, mul121666rax = mul121666rax * crypto_scalarmult_curve25519_amd64_64_121666;
	mul121666t1 = mul121666rax;
	mul121666t2 = mul121666rdx;
	mul121666rax = t53_stack;
	mul121666rdx, mul121666rax = mul121666rax * crypto_scalarmult_curve25519_amd64_64_121666;
	mul121666t3 = mul121666rax;
	mul121666r4 = mul121666rdx;
	cf, zp1 += mul121666t1;
	cf, zp2 += mul121666t2 + cf;
	cf, zp3 += mul121666t3 + cf;
	_, mul121666r4 += 0 + cf;
	mul121666r4 *= 38;
	cf, zp0 += mul121666r4;
	cf, zp1 += 0 + cf;
	cf, zp2 += 0 + cf;
	cf, zp3 += 0 + cf;
	mul121666t1 = 38;
	mul121666t2 = #x86_MOV(0);
	mul121666t1 = mul121666t2 if !cf;
	zp0 += mul121666t1;
	cf, zp0 += t70_stack;
	cf, zp1 += t71_stack + cf;
	cf, zp2 += t72_stack + cf;
	cf, zp3 += t73_stack + cf;
	addt0 = #x86_MOV(0);
	addt1 = 38;
	addt1 = addt0 if !cf;
	cf, zp0 += addt1;
	cf, zp1 += addt0 + cf;
	cf, zp2 += addt0 + cf;
	cf, zp3 += addt0 + cf;
	addt0 = addt1 if cf;
	zp0 += addt0;
	[workp + 8*8] = zp0;
	[workp + 9*8] = zp1;
	[workp + 10*8] = zp2;
	[workp + 11*8] = zp3;
	mulr4 = #x86_MOV(0);
	mulr5 = #x86_MOV(0);
	mulr6 = #x86_MOV(0);
	mulr7 = #x86_MOV(0);
	mulx0 = [workp + 8*8];
	mulrax = t50_stack;
	mulrdx, mulrax = mulrax * mulx0;
	zp0 = mulrax;
	zp1 = mulrdx;
	mulrax = t51_stack;
	mulrdx, mulrax = mulrax * mulx0;
	cf, zp1 += mulrax;
	zp2 = #x86_MOV(0);
	_, zp2 += mulrdx + cf;
	mulrax = t52_stack;
	mulrdx, mulrax = mulrax * mulx0;
	cf, zp2 += mulrax;
	zp3 = #x86_MOV(0);
	_, zp3 += mulrdx + cf;
	mulrax = t53_stack;
	mulrdx, mulrax = mulrax * mulx0;
	cf, zp3 += mulrax;
	_, mulr4 += mulrdx + cf;
	mulx1 = [workp + 9*8];
	mulrax = t50_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, zp1 += mulrax;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t51_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, zp2 += mulrax;
	_, mulrdx += 0 + cf;
	cf, zp2 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t52_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, zp3 += mulrax;
	_, mulrdx += 0 + cf;
	cf, zp3 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t53_stack;
	mulrdx, mulrax = mulrax * mulx1;
	cf, mulr4 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr4 += mulc;
	_, mulr5 += mulrdx + cf;
	mulx2 = [workp + 10*8];
	mulrax = t50_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, zp2 += mulrax;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t51_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, zp3 += mulrax;
	_, mulrdx += 0 + cf;
	cf, zp3 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t52_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, mulr4 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr4 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t53_stack;
	mulrdx, mulrax = mulrax * mulx2;
	cf, mulr5 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr5 += mulc;
	_, mulr6 += mulrdx + cf;
	mulx3 = [workp + 11*8];
	mulrax = t50_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, zp3 += mulrax;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t51_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, mulr4 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr4 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t52_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, mulr5 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr5 += mulc;
	mulc = #x86_MOV(0);
	_, mulc += mulrdx + cf;
	mulrax = t53_stack;
	mulrdx, mulrax = mulrax * mulx3;
	cf, mulr6 += mulrax;
	_, mulrdx += 0 + cf;
	cf, mulr6 += mulc;
	_, mulr7 += mulrdx + cf;
	mulrax = mulr4;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	mulr4 = mulrax;
	mulrax = mulr5;
	mulr5 = mulrdx;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, mulr5 += mulrax;
	mulrax = mulr6;
	mulr6 = #x86_MOV(0);
	_, mulr6 += mulrdx + cf;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, mulr6 += mulrax;
	mulrax = mulr7;
	mulr7 = #x86_MOV(0);
	_, mulr7 += mulrdx + cf;
	mulrdx, mulrax = mulrax * crypto_scalarmult_curve25519_amd64_64_38;
	cf, mulr7 += mulrax;
	mulr8 = #x86_MOV(0);
	_, mulr8 += mulrdx + cf;
	cf, zp0 += mulr4;
	cf, zp1 += mulr5 + cf;
	cf, zp2 += mulr6 + cf;
	cf, zp3 += mulr7 + cf;
	mulzero = #x86_MOV(0);
	_, mulr8 += mulzero + cf;
	mulr8 *= 38;
	cf, zp0 += mulr8;
	cf, zp1 += mulzero + cf;
	cf, zp2 += mulzero + cf;
	cf, zp3 += mulzero + cf;
	_, mulzero += mulzero + cf;
	mulzero *= 38;
	zp0 += mulzero;
	[workp + 8*8] = zp0;
	[workp + 9*8] = zp1;
	[workp + 10*8] = zp2;
	[workp + 11*8] = zp3;
	return;
}


