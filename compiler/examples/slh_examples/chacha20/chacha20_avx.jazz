// NOTES: this file expect an even CHACHA_ROUNDS param
param int CHACHA_ROUNDS=20;

u128 g_r16 = (16u8)[13,12,15,14, 9,8,11,10, 5,4,7,6, 1,0,3,2];
u128 g_r8  = (16u8)[14,13,12,15, 10,9,8,11, 6,5,4,7, 2,1,0,3];

u128 g_cnt      = (4u32)[3,2,1,0];
u128 g_cnt_inc  = (4u32)[4,4,4,4];

u128 g_p0       = 0;
u128 g_p1       = 1;

u128 g_sigma    = 0x6b20657479622d323320646e61707865;
u128 g_sigma0   = 0x61707865617078656170786561707865;
u128 g_sigma1   = 0x3320646e3320646e3320646e3320646e;
u128 g_sigma2   = 0x79622d3279622d3279622d3279622d32;
u128 g_sigma3   = 0x6b2065746b2065746b2065746b206574;

param int i_0 = 0;
param int i_1 = 1;
param int i_2 = 2;
param int i_3 = 3;
param int i_4 = 4;
param int i_5 = 5;
param int i_6 = 6;
param int i_7 = 7;
param int i_8 = 8;
param int i_9 = 9;
param int i_10 = 10;
param int i_11 = 11;
param int i_12 = 12;
param int i_13 = 13;
param int i_14 = 14;
param int i_15 = 15;
param int i_32 = 32;

inline fn __load_shufb_cmd_avx() -> stack u128, stack u128
{
  reg   u128   r16,   r8;
  stack u128 s_r16, s_r8;

  r16 = g_r16;
  r8 = g_r8;
  s_r16 = r16;
  s_r8 = r8;

  return s_r16, s_r8;
}

inline fn __init_x1_avx(reg u64 key nonce, reg u32 counter) -> reg u128[4]
{
  reg u128[4] st;

  st[0] = g_sigma;
  st[1] = (u128)[key + 0];
  st[2] = (u128)[key + 16];
  st[3] = g_p0;
  st[3] = #VPINSR_4u32(st[3], counter, 0);
  st[3] = #VPINSR_4u32(st[3], (u32)[nonce + 0], 1);
  st[3] = #VPINSR_2u64(st[3], (u64)[nonce + 4], 1);

  // st
  // 0 { sigma      }
  // 1 { k[127:0]   }
  // 2 { k[255:128] }
  // 3 { n , cnt    }
  return st;
}

inline fn __init_x4_avx(reg u64 k n1, reg u32 ctr) -> stack u128[16]
{
  inline int i;
  stack u128[16] st;
  reg u128[16] s;
  stack u32 s_ctr;

  s_ctr = ctr;

  s[0] =  g_sigma0;
  s[1] =  g_sigma1;
  s[2] =  g_sigma2;
  s[3] =  g_sigma3;

  for i=0 to 8
  { s[4+i] = #VPBROADCAST_4u32((u32)[k + 4*i]); }

  s[12] = #VPBROADCAST_4u32(s_ctr);
  s[12] +4u32= g_cnt;

  for i=0 to 3
  { s[13+i] = #VPBROADCAST_4u32((u32)[n1 + 4*i]); }

  st = s;

  //  st
  //  0 { ... , sigma0     , sigma0     }
  //  1 { ... , sigma1     , sigma1     }
  //  2 { ... , sigma2     , sigma2     }
  //  3 { ... , sigma3     , sigma3     }
  //  4 { ... , k[31:0]    , k[31:0]    }
  // ...
  // 11 { ... , k[255:224] , k[255:224] }
  // 12 { ... , ctr+1      , ctr        }
  // 13 { ... , n[31:0]    , n[31:0]    }
  // ...
  // 15 { ... , n[95:64]   , n[95:64]   }
  return st;
}

inline fn __copy_state_x1_avx(reg u128[4] st) -> reg u128[4]
{
  reg u128[4] k;
  k = st;
  return k;
}

inline fn __copy_state_x2_avx(reg u128[4] st) -> reg u128[4], reg u128[4]
{
  reg u128[4] k1 k2;
  k1 = st;
  k2 = st;
  k2[3] +4u32= g_p1;

  // k2             k1
  // { sigma      } { sigma      }
  // { k[127:0]   } { k[127:0]   }
  // { k[255:128] } { k[255:128] }
  // { n , cnt+1  } { n , cnt    }
  return k1, k2;
}

inline fn __copy_state_x4_avx(stack u128[16] st) -> reg u128[16]
{
  reg u128[16] k;
  k = st;
  return k;
}

inline fn __sum_states_x1_avx(reg u128[4] k st) -> reg u128[4]
{
  inline int i;
  for i=0 to 4
  { k[i] +4u32= st[i]; }
  return k;
}

inline fn __sum_states_x2_avx(reg u128[4] k1 k2 st) -> reg u128[4], reg u128[4]
{
  inline int i;
  k1 = __sum_states_x1_avx(k1, st);
  k2 = __sum_states_x1_avx(k2, st);
  k2[3] +4u32= g_p1;
  return k1, k2;
}

inline fn __sum_states_x4_avx(reg u128[16] k, stack u128[16] st) -> reg u128[16]
{
  inline int i;
  for i=0 to 16
  { k[i] +4u32= st[i]; }
  return k;
}

// t[0] = [17, 16,  1,  0]
// t[1] = [19, 18,  3,  2]
// t[2] = [21, 20,  5,  4]
// t[3] = [23, 22,  7,  6]
// t[4] = [49, 48, 33, 32]
// t[5] = [51, 50, 35, 34]
// t[6] = [53, 52, 37, 36]
// t[7] = [55, 54, 39, 38]
inline fn __sub_rotate_avx( reg u128[8] t) ->  reg u128[8]
{
  inline int i;
  reg u128[8] x;

  x[0] = #VPUNPCKL_2u64(t[0], t[1]);
  x[1] = #VPUNPCKL_2u64(t[2], t[3]);
  x[2] = #VPUNPCKH_2u64(t[0], t[1]);
  x[3] = #VPUNPCKH_2u64(t[2], t[3]);

  x[4] = #VPUNPCKL_2u64(t[4], t[5]);
  x[5] = #VPUNPCKL_2u64(t[6], t[7]);
  x[6] = #VPUNPCKH_2u64(t[4], t[5]);
  x[7] = #VPUNPCKH_2u64(t[6], t[7]);

  // x[0] = [3,   2,  1,  0] +0
  // x[1] = [7,   6,  5,  4] +16
  // x[2] = [19, 18, 17, 16] +64
  // x[3] = [23, 22, 21, 20] +80
  // x[4] = [35, 34, 33, 32] +128
  // x[5] = [39, 38, 37, 36] +144
  // x[6] = [51, 50, 49, 48] +192
  // x[7] = [55, 54, 53, 52] +208
  return x;
}

// x[0] = [48, 32, 16, 0]
// x[1] = [49, 33, 17, 1]
// x[2] = [50, 34, 18, 2]
// x[3] = [51, 35, 19, 3]
// x[4] = [52, 36, 20, 4]
// x[5] = [53, 37, 21, 5]
// x[6] = [54, 38, 22, 6]
// x[7] = [55, 39, 23, 7]
inline fn __rotate_avx( reg u128[8] x) ->  reg u128[8]
{
  inline int i;
  reg u128[8] t;

  for i=0 to 4
  { t[  i] = #VPUNPCKL_4u32(x[2*i+0], x[2*i+1]);
    t[4+i] = #VPUNPCKH_4u32(x[2*i+0], x[2*i+1]); }

  x = __sub_rotate_avx(t);

  return x;
}

inline fn __rotate_stack_avx( stack u128[8] s) ->  reg u128[8]
{
  inline int i;
  reg u128[8] t, x;

  for i=0 to 4
  { x[i] = s[2*i+0]; }

  for i=0 to 4
  { t[  i] = #VPUNPCKL_4u32(x[i], s[2*i+1]);
    t[4+i] = #VPUNPCKH_4u32(x[i], s[2*i+1]);
  }

  x = __sub_rotate_avx(t);

  return x;
}

inline fn __rotate_first_half_x8_avx( reg u128[16] k) ->  reg u128[8],  stack u128[8]
{
  inline int i;
  stack u128[8] s_k8_15;
  reg   u128[8] k0_7;

  for i=0 to 8
  { s_k8_15[i] = k[8 + i]; }

  for i=0 to 8
  { k0_7[i] = k[i]; }

  k0_7 = __rotate_avx(k0_7);

  return k0_7, s_k8_15;
}

inline fn __rotate_second_half_x8_avx( stack u128[8] s_k8_15) ->  reg u128[8]
{
  inline int i;
  reg u128[8] k8_15;
  k8_15 = __rotate_stack_avx(s_k8_15);
  return k8_15;
}

inline fn __interleave_avx( stack u128[8] s,  reg u128[8] k,  inline int o) ->  reg u128[8]
{
  inline int i;
  reg u128[8] sk;

  sk[0] = s[o + 0];
  sk[1] = s[o + 1];
  sk[2] = k[o + 0];
  sk[3] = k[o + 1];
  sk[4] = s[o + 2];
  sk[5] = s[o + 3];
  sk[6] = k[o + 2];
  sk[7] = k[o + 3];

  return sk;
}

inline fn __update_ptr_avx( reg u64 output plain,  reg u32 len,  inline int n) ->  reg u64,  reg u64,  reg u32
{
  output += n;
  plain += n;
  len -= n;
  return output, plain, len;
}

inline fn __store_avx( reg u64 output plain,  reg u32 len,  reg u128[2] k) ->  reg u64,  reg u64,  reg u32,  reg u128[2]
{
  k[0] ^= (u128)[plain +  0];
  k[1] ^= (u128)[plain + 16];
  (u128)[output +  0] = k[0];
  (u128)[output + 16] = k[1];

  output, plain, len = __update_ptr_avx(output, plain, len, 32);

  return output, plain, len, k;
}

// stores at most 32 bytes
inline fn __store_last_avx( reg u64 output plain,  reg u32 len,  reg u128[2] k)
{
  reg u128 r1;
  reg u64  r2;
  reg u8   r3;

  r1 = k[0];

  if(len >= 16)
  {
    r1 ^= (u128)[plain + 0];
    (u128)[output + 0] = r1;
    output, plain, len = __update_ptr_avx(output, plain, len, 16);
    r1 = k[1];
  }

  r2 = #VPEXTR_64(r1, 0);

  if(len >= 8)
  {
    r2 ^= (u64)[plain + 0];
    (u64)[output + 0] = r2;
    output, plain, len = __update_ptr_avx(output, plain, len, 8);
    r2 = #VPEXTR_64(r1, 1);
  }

  while(len > 0)
  {
    r3 = r2;
    r3 ^= (u8)[plain + 0];
    (u8)[output + 0] = r3;
    r2 >>= 8;

    output, plain, len = __update_ptr_avx(output, plain, len, 1);
  }
}

// stores 64 bytes
inline fn __store_x1_avx( reg u64 output plain,  reg u32 len,  reg u128[4] k) ->  reg u64,  reg u64,  reg u32,  reg u128[4]
{
  inline int i;

  for i=0 to 4
  { k[i] ^= (u128)[plain + 16*i]; }

  for i=0 to 4
  { (u128)[output + 16*i] = k[i]; }

  output, plain, len = __update_ptr_avx(output, plain, len, 64);

  return output, plain, len, k;
}

// stores at most 64 bytes
inline fn __store_x1_last_avx( reg u64 output plain,  reg u32 len,   reg u128[4] k)
{
  reg u128[2] r;

  r[0] = k[0];
  r[1] = k[1];

  if(len >= 32)
  {
    output, plain, len, r = __store_avx(output, plain, len, r);
    r[0] = k[2];
    r[1] = k[3];
  }

  __store_last_avx(output, plain, len, r);
}

inline fn __store_x2_avx( reg u64 output plain,  reg u32 len,  reg u128[8] k) ->  reg u64,  reg u64,  reg u32
{
  inline int i;

  for i=0 to 8
  { k[i] ^= (u128)[plain + 16*i]; }

  for i=0 to 8
  { (u128)[output + 16*i] = k[i]; }

  output, plain, len = __update_ptr_avx(output, plain, len, 128);

  return output, plain, len;
}

inline fn __store_x2_last_avx( reg u64 output plain,  reg u32 len,  reg u128[8] k)
{
  inline int i;
  reg u128[4] r;

  for i=0 to 4
  { r[i] = k[i]; }

  if(len >= 64)
  {
    output, plain, len, r = __store_x1_avx(output, plain, len, r);
    for i=0 to 4 { r[i] = k[i+4]; }
  }

  __store_x1_last_avx(output, plain, len, r);
}

inline fn __store_half_x4_avx( reg u64 output plain,  reg u32 len,  reg u128[8] k,  inline int o)
{
  inline int i;

  for i=0 to 4
  { k[2*i  ] ^= (u128)[plain + o + 64*i];
    k[2*i+1] ^= (u128)[plain + o + 64*i + 16]; }

  for i=0 to 4
  { (u128)[output + o + 64*i     ] = k[2*i  ];
    (u128)[output + o + 64*i + 16] = k[2*i+1]; }
}

inline fn __store_x4_avx( reg u64 output plain,  reg u32 len,  reg u128[16] k) ->  reg u64,  reg u64,  reg u32
{
  stack u128[8] s_k8_15;
  reg   u128[8] k0_7, k8_15;

  k0_7, s_k8_15 = __rotate_first_half_x8_avx(k);
  __store_half_x4_avx(output, plain, len, k0_7, i_0);
  k8_15 = __rotate_second_half_x8_avx(s_k8_15);
  __store_half_x4_avx(output, plain, len, k8_15, i_32);

  output, plain, len = __update_ptr_avx(output, plain, len, 256);

  return output, plain, len;
}

inline fn __store_x4_last_avx( reg u64 output plain,  reg u32 len,  reg u128[16] k)
{
  stack u128[8] s_k0_7 s_k8_15;
  reg   u128[8] k0_7 k8_15 i0_7;

  k0_7, s_k8_15 = __rotate_first_half_x8_avx(k);
  s_k0_7 = k0_7;
  k8_15 = __rotate_second_half_x8_avx(s_k8_15);
  i0_7 = __interleave_avx(s_k0_7, k8_15, i_0);

  if(len >= 128)
  {
    output, plain, len = __store_x2_avx(output, plain, len, i0_7);
    i0_7 = __interleave_avx(s_k0_7, k8_15, i_4);
  }

  __store_x2_last_avx(output, plain, len, i0_7);
}

inline fn __increment_counter_x4_avx( stack u128[16] s) ->  stack u128[16]
{
  reg u128 t;
  t = g_cnt_inc;
  t +4u32= s[12];
  s[12] = t;
  return s;
}

inline fn __rotate_x4_avx( reg u128[4] k,  inline int i r,  reg u128 r16 r8) ->  reg u128[4]
{
  // 4 of x4 is the number of lanes, not stream blocks
  reg u128 t;

  if(r==16){
    k[i] = #VPSHUFB_128(k[i], r16);

  } else { if (r==8) {
    k[i] = #VPSHUFB_128(k[i], r8);

  } else {
    t = k[i] <<4u32 r;
    k[i] = k[i] >>4u32 (32-r);
    k[i] ^= t;

  }}

  return k;
}

inline fn __line_x4_avx( reg u128[4] k,  inline int a b c r,  reg u128 r16 r8) ->  reg u128[4]
{
  k[a/4] +4u32= k[b/4];
  k[c/4] ^= k[a/4];
  k = __rotate_x4_avx(k, (c/4), r, r16, r8);
  return k;
}

inline fn __round_x1_avx( reg u128[4] k,  reg u128 r16 r8) ->  reg u128[4]
{
  k = __line_x4_avx(k, 0, 4, 12, 16, r16, r8);
  k = __line_x4_avx(k, 8, 12, 4, 12, r16, r8);
  k = __line_x4_avx(k, 0, 4, 12,  8, r16, r8);
  k = __line_x4_avx(k, 8, 12, 4,  7, r16, r8);
  return k;
}

inline fn __shuffle_state_x1_avx( reg u128[4] k) ->  reg u128[4]
{
  k[1] = #VPSHUFD_128(k[1], (4u2)[0,3,2,1]);
  k[2] = #VPSHUFD_128(k[2], (4u2)[1,0,3,2]);
  k[3] = #VPSHUFD_128(k[3], (4u2)[2,1,0,3]);
  return k;
}

inline fn __reverse_shuffle_state_x1_avx( reg u128[4] k) ->  reg u128[4]
{
  k[1] = #VPSHUFD_128(k[1], (4u2)[2,1,0,3]);
  k[2] = #VPSHUFD_128(k[2], (4u2)[1,0,3,2]);
  k[3] = #VPSHUFD_128(k[3], (4u2)[0,3,2,1]);
  return k;
}

inline fn __column_round_x1_avx( reg u128[4] k,  reg u128 r16 r8) ->  reg u128[4]
{
  k = __round_x1_avx(k, r16, r8);
  return k;
}

inline fn __diagonal_round_x1_avx( reg u128[4] k,  reg u128 r16 r8) ->  reg u128[4]
{
  k = __shuffle_state_x1_avx(k);
  k = __round_x1_avx(k, r16, r8);
  k = __reverse_shuffle_state_x1_avx(k);
  return k;
}

inline fn __rounds_x1_avx( reg u128[4] k) ->  reg u128[4]
{
   reg u64 c;
  reg u128 r16 r8;

  r16 = g_r16;
  r8 = g_r8;

  c = 0;
  while(c < (CHACHA_ROUNDS/2))
  {
    k = __column_round_x1_avx(k, r16, r8);
    k = __diagonal_round_x1_avx(k, r16, r8);
    c += 1;
  }

  return k;
}

inline fn __inlined_round_x2_avx( reg u128[4] k1 k2,  reg u128 r16 r8) ->  reg u128[4],  reg u128[4]
{
  reg u128 t1;

  k1[0] +4u32= k1[1];
  k2[0] +4u32= k2[1];

  k1[3] ^= k1[0];
  k2[3] ^= k2[0];

  k1 = __rotate_x4_avx(k1, 3, 16, r16, r8);
  k2 = __rotate_x4_avx(k2, 3, 16, r16, r8);

  k1[2] +4u32= k1[3];
  k2[2] +4u32= k2[3];

                          k1[1] ^= k1[2];
  // inlined rotate
  t1 = k1[1] <<4u32 12;
  k1[1] = k1[1] >>4u32 20;
                          k2[1] ^= k2[2];
  k1[1] ^= t1;

  k2 = __rotate_x4_avx(k2, 1, 12, r16, r8);

  k1[0] +4u32= k1[1];
  k2[0] +4u32= k2[1];

  k1[3] ^= k1[0];
  k2[3] ^= k2[0];

  k1 = __rotate_x4_avx(k1, 3, 8, r16, r8);
  k2 = __rotate_x4_avx(k2, 3, 8, r16, r8);

  k1[2] +4u32= k1[3];
  k2[2] +4u32= k2[3];

                          k1[1] ^= k1[2];
  // inlined rotate
  t1 = k1[1] <<4u32 7;
  k1[1] = k1[1] >>4u32 25;
                          k2[1] ^= k2[2];
  k1[1] ^= t1;

  k2 = __rotate_x4_avx(k2, 1, 7, r16, r8);

  return k1, k2;
}

inline fn __column_round_x2_avx( reg u128[4] k1 k2,  reg u128 r16 r8) ->  reg u128[4],  reg u128[4]
{
  k1, k2 = __inlined_round_x2_avx(k1, k2, r16, r8);
  return k1, k2;
}

inline fn __shuffle_state_x2_avx( reg u128[4] k1 k2) ->  reg u128[4],  reg u128[4]
{
  k1 = __shuffle_state_x1_avx(k1);
  k2 = __shuffle_state_x1_avx(k2);
  return k1, k2;
}

inline fn __reverse_shuffle_state_x2_avx( reg u128[4] k1 k2) ->  reg u128[4],  reg u128[4]
{
  k1 = __reverse_shuffle_state_x1_avx(k1);
  k2 = __reverse_shuffle_state_x1_avx(k2);
  return k1, k2;
}

inline fn __diagonal_round_x2_avx( reg u128[4] k1 k2,  reg u128 r16 r8) ->  reg u128[4],  reg u128[4]
{
  k1, k2 = __shuffle_state_x2_avx(k1, k2);
  k1, k2 = __inlined_round_x2_avx(k1, k2, r16, r8);
  k1, k2 = __reverse_shuffle_state_x2_avx(k1, k2);
  return k1, k2;
}

inline fn __rounds_x2_avx( reg u128[4] k1 k2) ->  reg u128[4],  reg u128[4]
{
   reg u64 c;
  reg u128 r16 r8;

  r16 = g_r16;
  r8 = g_r8;

  // do not use align in while;
  // do not use zf;
  // it decreases the performance here.
  c = 0;
  while(c < (CHACHA_ROUNDS/2)) {
    k1, k2 = __column_round_x2_avx(k1, k2, r16, r8);
    k1, k2 = __diagonal_round_x2_avx(k1, k2, r16, r8);
    c += 1;
  }

  return k1, k2;
}

inline fn __rotate_x4_s_avx( reg u128[16] k,  inline int i r,  stack u128 r16 r8) ->  reg u128[16]
{
  reg u128 t;

  if(r==16){
    k[i] = #VPSHUFB_128(k[i], r16);

  } else { if (r==8) {
    k[i] = #VPSHUFB_128(k[i], r8);

  } else {
    t = k[i] <<4u32 r;
    k[i] = k[i] >>4u32 (32-r);
    k[i] ^= t;

  }}

  return k;
}

inline fn __line_x4_v_avx( reg u128[16] k,
                           inline int a0 b0 c0 r0
                                     a1 b1 c1 r1,
                           stack u128 r16 r8) ->  reg u128[16]
{
  k[a0] +4u32= k[b0];
  k[a1] +4u32= k[b1];

  k[c0] ^= k[a0];
  k[c1] ^= k[a1];

  k = __rotate_x4_s_avx(k, c0, r0, r16, r8);
  k = __rotate_x4_s_avx(k, c1, r1, r16, r8);

  return k;
}

inline fn __double_quarter_round_x4_avx( reg u128[16] k,
                                         inline int a0 b0 c0 d0
                                                   a1 b1 c1 d1,
                                         stack u128 r16 r8) ->  reg u128[16]
{
  k = __line_x4_v_avx(k, a0, b0, d0, 16, a1, b1, d1, 16, r16, r8);
  k = __line_x4_v_avx(k, c0, d0, b0, 12, c1, d1, b1, 12, r16, r8);
  k = __line_x4_v_avx(k, a0, b0, d0, 8,  a1, b1, d1, 8,  r16, r8);
  k = __line_x4_v_avx(k, c0, d0, b0, 7,  c1, d1, b1, 7,  r16, r8);

  return k;
}

inline fn __column_round_x4_avx( reg u128[16] k,  stack u128 k15 s_r16 s_r8) ->  reg u128[16],  stack u128
{
  stack u128 k_;

  k = __double_quarter_round_x4_avx(k, 0, 4, 8,  12,
                                       2, 6, 10, 14, s_r16, s_r8);
  k[15] = k15;
  k_ = k[14];

  k = __double_quarter_round_x4_avx(k, 1, 5, 9,  13,
                                       3, 7, 11, 15, s_r16, s_r8);
  return k, k_;
}

inline fn __diagonal_round_x4_avx( reg u128[16] k,  stack u128 k14 s_r16 s_r8) ->  reg u128[16],  stack u128
{
  stack u128 k_;

  k = __double_quarter_round_x4_avx(k, 1, 6, 11, 12,
                                       0, 5, 10, 15, s_r16, s_r8);
  k[14] = k14;
  k_ = k[15];

  k = __double_quarter_round_x4_avx(k, 2, 7, 8, 13,
                                       3, 4, 9, 14, s_r16, s_r8);
  return k, k_;
}

inline fn __rounds_x4_avx( reg u128[16] k,  stack u128 s_r16 s_r8) ->  reg u128[16]
{
   reg u64 c;
  //reg bool zf;
  stack u128 k14, k15;

  k15 = k[15];

  c = (CHACHA_ROUNDS/2);
  /*align*/ while {
    k, k14 = __column_round_x4_avx(k, k15, s_r16, s_r8);
    k, k15 = __diagonal_round_x4_avx(k, k14, s_r16, s_r8);
    (_,_,_,_,c) = #DEC(c);
  } (c > 0)

  k[15] = k15;

  return k;
}

inline fn __chacha_more_than_128_avx( reg u64 output plain,  reg u32 len,  reg u64 key nonce,  reg u32 counter)
{
  stack u128[16] st;
  reg   u128[16]  k;
  stack u128 s_r16 s_r8;

  s_r16, s_r8 = __load_shufb_cmd_avx();

  st = __init_x4_avx(key, nonce, counter);

  while(len >= 256)
  {
    k = __copy_state_x4_avx(st);
    k = __rounds_x4_avx(k, s_r16, s_r8);
    k = __sum_states_x4_avx(k, st);
    output, plain, len = __store_x4_avx(output, plain, len, k);
    st = __increment_counter_x4_avx(st);
  }

  if(len > 0)
  {
    k = __copy_state_x4_avx(st);
    k = __rounds_x4_avx(k, s_r16, s_r8);
    k = __sum_states_x4_avx(k, st);
    __store_x4_last_avx(output, plain, len, k);
  }
}

inline fn __chacha_less_than_129_avx( reg u64 output plain,  reg u32 len,  reg u64 key nonce,  reg u32 counter)
{
  reg u128[4] st k1, k2;

  st = __init_x1_avx(key, nonce, counter);

  if(len > 64)
  {
    k1, k2 = __copy_state_x2_avx(st);
    k1, k2 = __rounds_x2_avx(k1, k2);
    k1, k2 = __sum_states_x2_avx(k1, k2, st);

    output, plain, len, k1 = __store_x1_avx(output, plain, len, k1);
                             __store_x1_last_avx(output, plain, len, k2);
  }
  else
  {
    k1 = __copy_state_x1_avx(st);
    k1 = __rounds_x1_avx(k1);
    k1 = __sum_states_x1_avx(k1, st);
    __store_x1_last_avx(output, plain, len, k1);
  }
}

export fn chacha_avx(#transient reg u64 output plain, #transient reg u32 len, #transient reg u64 key nonce,  reg u32 counter)
{
  #msf reg u64 ms;
  ms = #init_msf();   // LFENCE
  if(len < 129) {
    __chacha_less_than_129_avx(output, plain, len, key, nonce, counter);
  } else {
    __chacha_more_than_128_avx(output, plain, len, key, nonce, counter);
  }

}
