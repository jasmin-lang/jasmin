param int crypto_scalarmult_curve25519_amd64_51_121666_213 = 996687872;
param int crypto_scalarmult_curve25519_amd64_51_2P0 = 0xFFFFFFFFFFFDA;
param int crypto_scalarmult_curve25519_amd64_51_2P1234 = 0xFFFFFFFFFFFFE;
param int crypto_scalarmult_curve25519_amd64_51_REDMASK51 = 0x0007FFFFFFFFFFFF;
export fn crypto_scalarmult_curve25519_amd64_51_ladderstep(reg u64 workp){
	 reg bool cf;
	 reg u64 mul121666rax;
	 reg u64 mul121666rdx;
	 reg u64 mulr01;
	 reg u64 mulr11;
	 reg u64 mulr21;
	 reg u64 mulr31;
	 reg u64 mulr41;
	 reg u64 mulrax;
	 reg u64 mulrdx;
	 reg u64 mulredmask;
	 reg u64 mult;
	 stack u64 mulx319_stack;
	 stack u64 mulx419_stack;
	 reg u64 squarer01;
	 reg u64 squarer11;
	 reg u64 squarer21;
	 reg u64 squarer31;
	 reg u64 squarer41;
	 reg u64 squarerax;
	 reg u64 squarerdx;
	 reg u64 squareredmask;
	 reg u64 squaret;
	 reg u64 t10;
	 stack u64 t10_stack;
	 reg u64 t11;
	 stack u64 t11_stack;
	 reg u64 t12;
	 stack u64 t12_stack;
	 reg u64 t13;
	 stack u64 t13_stack;
	 reg u64 t14;
	 stack u64 t14_stack;
	 reg u64 t20;
	 stack u64 t20_stack;
	 reg u64 t21;
	 stack u64 t21_stack;
	 reg u64 t22;
	 stack u64 t22_stack;
	 reg u64 t23;
	 stack u64 t23_stack;
	 reg u64 t24;
	 stack u64 t24_stack;
	 reg u64 t30;
	 stack u64 t30_stack;
	 reg u64 t31;
	 stack u64 t31_stack;
	 reg u64 t32;
	 stack u64 t32_stack;
	 reg u64 t33;
	 stack u64 t33_stack;
	 reg u64 t34;
	 stack u64 t34_stack;
	 reg u64 t40;
	 stack u64 t40_stack;
	 reg u64 t41;
	 stack u64 t41_stack;
	 reg u64 t42;
	 stack u64 t42_stack;
	 reg u64 t43;
	 stack u64 t43_stack;
	 reg u64 t44;
	 stack u64 t44_stack;
	 reg u64 t50;
	 stack u64 t50_stack;
	 reg u64 t51;
	 stack u64 t51_stack;
	 reg u64 t52;
	 stack u64 t52_stack;
	 reg u64 t53;
	 stack u64 t53_stack;
	 reg u64 t54;
	 stack u64 t54_stack;
	 reg u64 t60;
	 stack u64 t60_stack;
	 reg u64 t61;
	 stack u64 t61_stack;
	 reg u64 t62;
	 stack u64 t62_stack;
	 reg u64 t63;
	 stack u64 t63_stack;
	 reg u64 t64;
	 stack u64 t64_stack;
	 reg u64 t70;
	 stack u64 t70_stack;
	 reg u64 t71;
	 stack u64 t71_stack;
	 reg u64 t72;
	 stack u64 t72_stack;
	 reg u64 t73;
	 stack u64 t73_stack;
	 reg u64 t74;
	 stack u64 t74_stack;
	 reg u64 t80;
	 reg u64 t81;
	 reg u64 t82;
	 reg u64 t83;
	 reg u64 t84;
	 reg u64 t90;
	 stack u64 t90_stack;
	 reg u64 t91;
	 stack u64 t91_stack;
	 reg u64 t92;
	 stack u64 t92_stack;
	 reg u64 t93;
	 stack u64 t93_stack;
	 reg u64 t94;
	 stack u64 t94_stack;
	 reg u64 xp0;
	 reg u64 xp1;
	 reg u64 xp2;
	 reg u64 xp3;
	 reg u64 xp4;
	 reg u64 xq0;
	 reg u64 xq1;
	 reg u64 xq2;
	 reg u64 xq3;
	 reg u64 xq4;
	 reg u64 zp0;
	 reg u64 zp1;
	 reg u64 zp2;
	 reg u64 zp3;
	 reg u64 zp4;
	 reg u64 zq0;
	 reg u64 zq1;
	 reg u64 zq2;
	 reg u64 zq3;
	 reg u64 zq4;

	t10 = [workp + 5*8];
	t11 = [workp + 6*8];
	t12 = [workp + 7*8];
	t13 = [workp + 8*8];
	t14 = [workp + 9*8];
	t20 = t10;
	t21 = t11;
	t22 = t12;
	t23 = t13;
	t24 = t14;
	t20 += crypto_scalarmult_curve25519_amd64_51_2P0;
	t21 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	t22 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	t23 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	t24 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	t10 += [workp + 10*8];
	t11 += [workp + 11*8];
	t12 += [workp + 12*8];
	t13 += [workp + 13*8];
	t14 += [workp + 14*8];
	t20 -= [workp + 10*8];
	t21 -= [workp + 11*8];
	t22 -= [workp + 12*8];
	t23 -= [workp + 13*8];
	t24 -= [workp + 14*8];
	t10_stack = t10;
	t11_stack = t11;
	t12_stack = t12;
	t13_stack = t13;
	t14_stack = t14;
	t20_stack = t20;
	t21_stack = t21;
	t22_stack = t22;
	t23_stack = t23;
	t24_stack = t24;
	squarerax = t20_stack;
	squarerdx, squarerax = squarerax * t20_stack;
	t70 = squarerax;
	squarer01 = squarerdx;
	squarerax = t20_stack;
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * t21_stack;
	t71 = squarerax;
	squarer11 = squarerdx;
	squarerax = t20_stack;
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * t22_stack;
	t72 = squarerax;
	squarer21 = squarerdx;
	squarerax = t20_stack;
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * t23_stack;
	t73 = squarerax;
	squarer31 = squarerdx;
	squarerax = t20_stack;
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * t24_stack;
	t74 = squarerax;
	squarer41 = squarerdx;
	squarerax = t21_stack;
	squarerdx, squarerax = squarerax * t21_stack;
	cf, t72 += squarerax;
	_, squarer21 += squarerdx + cf;
	squarerax = t21_stack;
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * t22_stack;
	cf, t73 += squarerax;
	_, squarer31 += squarerdx + cf;
	squarerax = t21_stack;
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * t23_stack;
	cf, t74 += squarerax;
	_, squarer41 += squarerdx + cf;
	squarerax = t21_stack;
	squarerax *= 38;
	squarerdx, squarerax = squarerax * t24_stack;
	cf, t70 += squarerax;
	_, squarer01 += squarerdx + cf;
	squarerax = t22_stack;
	squarerdx, squarerax = squarerax * t22_stack;
	cf, t74 += squarerax;
	_, squarer41 += squarerdx + cf;
	squarerax = t22_stack;
	squarerax *= 38;
	squarerdx, squarerax = squarerax * t23_stack;
	cf, t70 += squarerax;
	_, squarer01 += squarerdx + cf;
	squarerax = t22_stack;
	squarerax *= 38;
	squarerdx, squarerax = squarerax * t24_stack;
	cf, t71 += squarerax;
	_, squarer11 += squarerdx + cf;
	squarerax = t23_stack;
	squarerax *= 19;
	squarerdx, squarerax = squarerax * t23_stack;
	cf, t71 += squarerax;
	_, squarer11 += squarerdx + cf;
	squarerax = t23_stack;
	squarerax *= 38;
	squarerdx, squarerax = squarerax * t24_stack;
	cf, t72 += squarerax;
	_, squarer21 += squarerdx + cf;
	squarerax = t24_stack;
	squarerax *= 19;
	squarerdx, squarerax = squarerax * t24_stack;
	cf, t73 += squarerax;
	_, squarer31 += squarerdx + cf;
	squareredmask = crypto_scalarmult_curve25519_amd64_51_REDMASK51;
	_, _, _, _, _, squarer01 = #x86_SHLD(squarer01, t70, 13);
	t70 &= squareredmask;
	_, _, _, _, _, squarer11 = #x86_SHLD(squarer11, t71, 13);
	t71 &= squareredmask;
	t71 += squarer01;
	_, _, _, _, _, squarer21 = #x86_SHLD(squarer21, t72, 13);
	t72 &= squareredmask;
	t72 += squarer11;
	_, _, _, _, _, squarer31 = #x86_SHLD(squarer31, t73, 13);
	t73 &= squareredmask;
	t73 += squarer21;
	_, _, _, _, _, squarer41 = #x86_SHLD(squarer41, t74, 13);
	t74 &= squareredmask;
	t74 += squarer31;
	squarer41 = squarer41 * 19;
	t70 += squarer41;
	squaret = t70;
	squaret >>= 51;
	squaret += t71;
	t70 &= squareredmask;
	t71 = squaret;
	squaret >>= 51;
	squaret += t72;
	t71 &= squareredmask;
	t72 = squaret;
	squaret >>= 51;
	squaret += t73;
	t72 &= squareredmask;
	t73 = squaret;
	squaret >>= 51;
	squaret += t74;
	t73 &= squareredmask;
	t74 = squaret;
	squaret >>= 51;
	squaret *= 19;
	t70 += squaret;
	t74 &= squareredmask;
	t70_stack = t70;
	t71_stack = t71;
	t72_stack = t72;
	t73_stack = t73;
	t74_stack = t74;
	squarerax = t10_stack;
	squarerdx, squarerax = squarerax * t10_stack;
	t60 = squarerax;
	squarer01 = squarerdx;
	squarerax = t10_stack;
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * t11_stack;
	t61 = squarerax;
	squarer11 = squarerdx;
	squarerax = t10_stack;
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * t12_stack;
	t62 = squarerax;
	squarer21 = squarerdx;
	squarerax = t10_stack;
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * t13_stack;
	t63 = squarerax;
	squarer31 = squarerdx;
	squarerax = t10_stack;
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * t14_stack;
	t64 = squarerax;
	squarer41 = squarerdx;
	squarerax = t11_stack;
	squarerdx, squarerax = squarerax * t11_stack;
	cf, t62 += squarerax;
	_, squarer21 += squarerdx + cf;
	squarerax = t11_stack;
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * t12_stack;
	cf, t63 += squarerax;
	_, squarer31 += squarerdx + cf;
	squarerax = t11_stack;
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * t13_stack;
	cf, t64 += squarerax;
	_, squarer41 += squarerdx + cf;
	squarerax = t11_stack;
	squarerax *= 38;
	squarerdx, squarerax = squarerax * t14_stack;
	cf, t60 += squarerax;
	_, squarer01 += squarerdx + cf;
	squarerax = t12_stack;
	squarerdx, squarerax = squarerax * t12_stack;
	cf, t64 += squarerax;
	_, squarer41 += squarerdx + cf;
	squarerax = t12_stack;
	squarerax *= 38;
	squarerdx, squarerax = squarerax * t13_stack;
	cf, t60 += squarerax;
	_, squarer01 += squarerdx + cf;
	squarerax = t12_stack;
	squarerax *= 38;
	squarerdx, squarerax = squarerax * t14_stack;
	cf, t61 += squarerax;
	_, squarer11 += squarerdx + cf;
	squarerax = t13_stack;
	squarerax *= 19;
	squarerdx, squarerax = squarerax * t13_stack;
	cf, t61 += squarerax;
	_, squarer11 += squarerdx + cf;
	squarerax = t13_stack;
	squarerax *= 38;
	squarerdx, squarerax = squarerax * t14_stack;
	cf, t62 += squarerax;
	_, squarer21 += squarerdx + cf;
	squarerax = t14_stack;
	squarerax *= 19;
	squarerdx, squarerax = squarerax * t14_stack;
	cf, t63 += squarerax;
	_, squarer31 += squarerdx + cf;
	squareredmask = crypto_scalarmult_curve25519_amd64_51_REDMASK51;
	_, _, _, _, _, squarer01 = #x86_SHLD(squarer01, t60, 13);
	t60 &= squareredmask;
	_, _, _, _, _, squarer11 = #x86_SHLD(squarer11, t61, 13);
	t61 &= squareredmask;
	t61 += squarer01;
	_, _, _, _, _, squarer21 = #x86_SHLD(squarer21, t62, 13);
	t62 &= squareredmask;
	t62 += squarer11;
	_, _, _, _, _, squarer31 = #x86_SHLD(squarer31, t63, 13);
	t63 &= squareredmask;
	t63 += squarer21;
	_, _, _, _, _, squarer41 = #x86_SHLD(squarer41, t64, 13);
	t64 &= squareredmask;
	t64 += squarer31;
	squarer41 = squarer41 * 19;
	t60 += squarer41;
	squaret = t60;
	squaret >>= 51;
	squaret += t61;
	t60 &= squareredmask;
	t61 = squaret;
	squaret >>= 51;
	squaret += t62;
	t61 &= squareredmask;
	t62 = squaret;
	squaret >>= 51;
	squaret += t63;
	t62 &= squareredmask;
	t63 = squaret;
	squaret >>= 51;
	squaret += t64;
	t63 &= squareredmask;
	t64 = squaret;
	squaret >>= 51;
	squaret *= 19;
	t60 += squaret;
	t64 &= squareredmask;
	t60_stack = t60;
	t61_stack = t61;
	t62_stack = t62;
	t63_stack = t63;
	t64_stack = t64;
	t50 = t60;
	t51 = t61;
	t52 = t62;
	t53 = t63;
	t54 = t64;
	t50 += crypto_scalarmult_curve25519_amd64_51_2P0;
	t51 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	t52 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	t53 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	t54 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	t50 -= t70_stack;
	t51 -= t71_stack;
	t52 -= t72_stack;
	t53 -= t73_stack;
	t54 -= t74_stack;
	t50_stack = t50;
	t51_stack = t51;
	t52_stack = t52;
	t53_stack = t53;
	t54_stack = t54;
	t30 = [workp + 15*8];
	t31 = [workp + 16*8];
	t32 = [workp + 17*8];
	t33 = [workp + 18*8];
	t34 = [workp + 19*8];
	t40 = t30;
	t41 = t31;
	t42 = t32;
	t43 = t33;
	t44 = t34;
	t40 += crypto_scalarmult_curve25519_amd64_51_2P0;
	t41 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	t42 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	t43 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	t44 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	t30 += [workp + 20*8];
	t31 += [workp + 21*8];
	t32 += [workp + 22*8];
	t33 += [workp + 23*8];
	t34 += [workp + 24*8];
	t40 -= [workp + 20*8];
	t41 -= [workp + 21*8];
	t42 -= [workp + 22*8];
	t43 -= [workp + 23*8];
	t44 -= [workp + 24*8];
	t30_stack = t30;
	t31_stack = t31;
	t32_stack = t32;
	t33_stack = t33;
	t34_stack = t34;
	t40_stack = t40;
	t41_stack = t41;
	t42_stack = t42;
	t43_stack = t43;
	t44_stack = t44;
	mulrax = t33_stack;
	mulrax *= 19;
	mulx319_stack = mulrax;
	mulrdx, mulrax = mulrax * t22_stack;
	t90 = mulrax;
	mulr01 = mulrdx;
	mulrax = t34_stack;
	mulrax *= 19;
	mulx419_stack = mulrax;
	mulrdx, mulrax = mulrax * t21_stack;
	cf, t90 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = t30_stack;
	mulrdx, mulrax = mulrax * t20_stack;
	cf, t90 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = t30_stack;
	mulrdx, mulrax = mulrax * t21_stack;
	t91 = mulrax;
	mulr11 = mulrdx;
	mulrax = t30_stack;
	mulrdx, mulrax = mulrax * t22_stack;
	t92 = mulrax;
	mulr21 = mulrdx;
	mulrax = t30_stack;
	mulrdx, mulrax = mulrax * t23_stack;
	t93 = mulrax;
	mulr31 = mulrdx;
	mulrax = t30_stack;
	mulrdx, mulrax = mulrax * t24_stack;
	t94 = mulrax;
	mulr41 = mulrdx;
	mulrax = t31_stack;
	mulrdx, mulrax = mulrax * t20_stack;
	cf, t91 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = t31_stack;
	mulrdx, mulrax = mulrax * t21_stack;
	cf, t92 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = t31_stack;
	mulrdx, mulrax = mulrax * t22_stack;
	cf, t93 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulrax = t31_stack;
	mulrdx, mulrax = mulrax * t23_stack;
	cf, t94 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = t31_stack;
	mulrax *= 19;
	mulrdx, mulrax = mulrax * t24_stack;
	cf, t90 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = t32_stack;
	mulrdx, mulrax = mulrax * t20_stack;
	cf, t92 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = t32_stack;
	mulrdx, mulrax = mulrax * t21_stack;
	cf, t93 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulrax = t32_stack;
	mulrdx, mulrax = mulrax * t22_stack;
	cf, t94 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = t32_stack;
	mulrax *= 19;
	mulrdx, mulrax = mulrax * t23_stack;
	cf, t90 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = t32_stack;
	mulrax *= 19;
	mulrdx, mulrax = mulrax * t24_stack;
	cf, t91 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = t33_stack;
	mulrdx, mulrax = mulrax * t20_stack;
	cf, t93 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulrax = t33_stack;
	mulrdx, mulrax = mulrax * t21_stack;
	cf, t94 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = mulx319_stack;
	mulrdx, mulrax = mulrax * t23_stack;
	cf, t91 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = mulx319_stack;
	mulrdx, mulrax = mulrax * t24_stack;
	cf, t92 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = t34_stack;
	mulrdx, mulrax = mulrax * t20_stack;
	cf, t94 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = mulx419_stack;
	mulrdx, mulrax = mulrax * t22_stack;
	cf, t91 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = mulx419_stack;
	mulrdx, mulrax = mulrax * t23_stack;
	cf, t92 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = mulx419_stack;
	mulrdx, mulrax = mulrax * t24_stack;
	cf, t93 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulredmask = crypto_scalarmult_curve25519_amd64_51_REDMASK51;
	_, _, _, _, _, mulr01 = #x86_SHLD(mulr01, t90, 13);
	t90 &= mulredmask;
	_, _, _, _, _, mulr11 = #x86_SHLD(mulr11, t91, 13);
	t91 &= mulredmask;
	t91 += mulr01;
	_, _, _, _, _, mulr21 = #x86_SHLD(mulr21, t92, 13);
	t92 &= mulredmask;
	t92 += mulr11;
	_, _, _, _, _, mulr31 = #x86_SHLD(mulr31, t93, 13);
	t93 &= mulredmask;
	t93 += mulr21;
	_, _, _, _, _, mulr41 = #x86_SHLD(mulr41, t94, 13);
	t94 &= mulredmask;
	t94 += mulr31;
	mulr41 = mulr41 * 19;
	t90 += mulr41;
	mult = t90;
	mult >>= 51;
	mult += t91;
	t91 = mult;
	mult >>= 51;
	t90 &= mulredmask;
	mult += t92;
	t92 = mult;
	mult >>= 51;
	t91 &= mulredmask;
	mult += t93;
	t93 = mult;
	mult >>= 51;
	t92 &= mulredmask;
	mult += t94;
	t94 = mult;
	mult >>= 51;
	t93 &= mulredmask;
	mult *= 19;
	t90 += mult;
	t94 &= mulredmask;
	t90_stack = t90;
	t91_stack = t91;
	t92_stack = t92;
	t93_stack = t93;
	t94_stack = t94;
	mulrax = t43_stack;
	mulrax *= 19;
	mulx319_stack = mulrax;
	mulrdx, mulrax = mulrax * t12_stack;
	t80 = mulrax;
	mulr01 = mulrdx;
	mulrax = t44_stack;
	mulrax *= 19;
	mulx419_stack = mulrax;
	mulrdx, mulrax = mulrax * t11_stack;
	cf, t80 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = t40_stack;
	mulrdx, mulrax = mulrax * t10_stack;
	cf, t80 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = t40_stack;
	mulrdx, mulrax = mulrax * t11_stack;
	t81 = mulrax;
	mulr11 = mulrdx;
	mulrax = t40_stack;
	mulrdx, mulrax = mulrax * t12_stack;
	t82 = mulrax;
	mulr21 = mulrdx;
	mulrax = t40_stack;
	mulrdx, mulrax = mulrax * t13_stack;
	t83 = mulrax;
	mulr31 = mulrdx;
	mulrax = t40_stack;
	mulrdx, mulrax = mulrax * t14_stack;
	t84 = mulrax;
	mulr41 = mulrdx;
	mulrax = t41_stack;
	mulrdx, mulrax = mulrax * t10_stack;
	cf, t81 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = t41_stack;
	mulrdx, mulrax = mulrax * t11_stack;
	cf, t82 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = t41_stack;
	mulrdx, mulrax = mulrax * t12_stack;
	cf, t83 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulrax = t41_stack;
	mulrdx, mulrax = mulrax * t13_stack;
	cf, t84 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = t41_stack;
	mulrax *= 19;
	mulrdx, mulrax = mulrax * t14_stack;
	cf, t80 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = t42_stack;
	mulrdx, mulrax = mulrax * t10_stack;
	cf, t82 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = t42_stack;
	mulrdx, mulrax = mulrax * t11_stack;
	cf, t83 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulrax = t42_stack;
	mulrdx, mulrax = mulrax * t12_stack;
	cf, t84 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = t42_stack;
	mulrax *= 19;
	mulrdx, mulrax = mulrax * t13_stack;
	cf, t80 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = t42_stack;
	mulrax *= 19;
	mulrdx, mulrax = mulrax * t14_stack;
	cf, t81 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = t43_stack;
	mulrdx, mulrax = mulrax * t10_stack;
	cf, t83 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulrax = t43_stack;
	mulrdx, mulrax = mulrax * t11_stack;
	cf, t84 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = mulx319_stack;
	mulrdx, mulrax = mulrax * t13_stack;
	cf, t81 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = mulx319_stack;
	mulrdx, mulrax = mulrax * t14_stack;
	cf, t82 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = t44_stack;
	mulrdx, mulrax = mulrax * t10_stack;
	cf, t84 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = mulx419_stack;
	mulrdx, mulrax = mulrax * t12_stack;
	cf, t81 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = mulx419_stack;
	mulrdx, mulrax = mulrax * t13_stack;
	cf, t82 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = mulx419_stack;
	mulrdx, mulrax = mulrax * t14_stack;
	cf, t83 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulredmask = crypto_scalarmult_curve25519_amd64_51_REDMASK51;
	_, _, _, _, _, mulr01 = #x86_SHLD(mulr01, t80, 13);
	t80 &= mulredmask;
	_, _, _, _, _, mulr11 = #x86_SHLD(mulr11, t81, 13);
	t81 &= mulredmask;
	t81 += mulr01;
	_, _, _, _, _, mulr21 = #x86_SHLD(mulr21, t82, 13);
	t82 &= mulredmask;
	t82 += mulr11;
	_, _, _, _, _, mulr31 = #x86_SHLD(mulr31, t83, 13);
	t83 &= mulredmask;
	t83 += mulr21;
	_, _, _, _, _, mulr41 = #x86_SHLD(mulr41, t84, 13);
	t84 &= mulredmask;
	t84 += mulr31;
	mulr41 = mulr41 * 19;
	t80 += mulr41;
	mult = t80;
	mult >>= 51;
	mult += t81;
	t81 = mult;
	mult >>= 51;
	t80 &= mulredmask;
	mult += t82;
	t82 = mult;
	mult >>= 51;
	t81 &= mulredmask;
	mult += t83;
	t83 = mult;
	mult >>= 51;
	t82 &= mulredmask;
	mult += t84;
	t84 = mult;
	mult >>= 51;
	t83 &= mulredmask;
	mult *= 19;
	t80 += mult;
	t84 &= mulredmask;
	zq0 = t80;
	zq1 = t81;
	zq2 = t82;
	zq3 = t83;
	zq4 = t84;
	zq0 += crypto_scalarmult_curve25519_amd64_51_2P0;
	zq1 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	zq2 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	zq3 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	zq4 += crypto_scalarmult_curve25519_amd64_51_2P1234;
	t80 += t90_stack;
	t81 += t91_stack;
	t82 += t92_stack;
	t83 += t93_stack;
	t84 += t94_stack;
	zq0 -= t90_stack;
	zq1 -= t91_stack;
	zq2 -= t92_stack;
	zq3 -= t93_stack;
	zq4 -= t94_stack;
	[workp + 15*8] = t80;
	[workp + 16*8] = t81;
	[workp + 17*8] = t82;
	[workp + 18*8] = t83;
	[workp + 19*8] = t84;
	[workp + 20*8] = zq0;
	[workp + 21*8] = zq1;
	[workp + 22*8] = zq2;
	[workp + 23*8] = zq3;
	[workp + 24*8] = zq4;
	squarerax = [workp + 15*8];
	squarerdx, squarerax = squarerax * [workp + 15*8];
	xq0 = squarerax;
	squarer01 = squarerdx;
	squarerax = [workp + 15*8];
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * [workp + 16*8];
	xq1 = squarerax;
	squarer11 = squarerdx;
	squarerax = [workp + 15*8];
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * [workp + 17*8];
	xq2 = squarerax;
	squarer21 = squarerdx;
	squarerax = [workp + 15*8];
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * [workp + 18*8];
	xq3 = squarerax;
	squarer31 = squarerdx;
	squarerax = [workp + 15*8];
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * [workp + 19*8];
	xq4 = squarerax;
	squarer41 = squarerdx;
	squarerax = [workp + 16*8];
	squarerdx, squarerax = squarerax * [workp + 16*8];
	cf, xq2 += squarerax;
	_, squarer21 += squarerdx + cf;
	squarerax = [workp + 16*8];
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * [workp + 17*8];
	cf, xq3 += squarerax;
	_, squarer31 += squarerdx + cf;
	squarerax = [workp + 16*8];
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * [workp + 18*8];
	cf, xq4 += squarerax;
	_, squarer41 += squarerdx + cf;
	squarerax = [workp + 16*8];
	squarerax *= 38;
	squarerdx, squarerax = squarerax * [workp + 19*8];
	cf, xq0 += squarerax;
	_, squarer01 += squarerdx + cf;
	squarerax = [workp + 17*8];
	squarerdx, squarerax = squarerax * [workp + 17*8];
	cf, xq4 += squarerax;
	_, squarer41 += squarerdx + cf;
	squarerax = [workp + 17*8];
	squarerax *= 38;
	squarerdx, squarerax = squarerax * [workp + 18*8];
	cf, xq0 += squarerax;
	_, squarer01 += squarerdx + cf;
	squarerax = [workp + 17*8];
	squarerax *= 38;
	squarerdx, squarerax = squarerax * [workp + 19*8];
	cf, xq1 += squarerax;
	_, squarer11 += squarerdx + cf;
	squarerax = [workp + 18*8];
	squarerax *= 19;
	squarerdx, squarerax = squarerax * [workp + 18*8];
	cf, xq1 += squarerax;
	_, squarer11 += squarerdx + cf;
	squarerax = [workp + 18*8];
	squarerax *= 38;
	squarerdx, squarerax = squarerax * [workp + 19*8];
	cf, xq2 += squarerax;
	_, squarer21 += squarerdx + cf;
	squarerax = [workp + 19*8];
	squarerax *= 19;
	squarerdx, squarerax = squarerax * [workp + 19*8];
	cf, xq3 += squarerax;
	_, squarer31 += squarerdx + cf;
	squareredmask = crypto_scalarmult_curve25519_amd64_51_REDMASK51;
	_, _, _, _, _, squarer01 = #x86_SHLD(squarer01, xq0, 13);
	xq0 &= squareredmask;
	_, _, _, _, _, squarer11 = #x86_SHLD(squarer11, xq1, 13);
	xq1 &= squareredmask;
	xq1 += squarer01;
	_, _, _, _, _, squarer21 = #x86_SHLD(squarer21, xq2, 13);
	xq2 &= squareredmask;
	xq2 += squarer11;
	_, _, _, _, _, squarer31 = #x86_SHLD(squarer31, xq3, 13);
	xq3 &= squareredmask;
	xq3 += squarer21;
	_, _, _, _, _, squarer41 = #x86_SHLD(squarer41, xq4, 13);
	xq4 &= squareredmask;
	xq4 += squarer31;
	squarer41 = squarer41 * 19;
	xq0 += squarer41;
	squaret = xq0;
	squaret >>= 51;
	squaret += xq1;
	xq0 &= squareredmask;
	xq1 = squaret;
	squaret >>= 51;
	squaret += xq2;
	xq1 &= squareredmask;
	xq2 = squaret;
	squaret >>= 51;
	squaret += xq3;
	xq2 &= squareredmask;
	xq3 = squaret;
	squaret >>= 51;
	squaret += xq4;
	xq3 &= squareredmask;
	xq4 = squaret;
	squaret >>= 51;
	squaret *= 19;
	xq0 += squaret;
	xq4 &= squareredmask;
	[workp + 15*8] = xq0;
	[workp + 16*8] = xq1;
	[workp + 17*8] = xq2;
	[workp + 18*8] = xq3;
	[workp + 19*8] = xq4;
	squarerax = [workp + 20*8];
	squarerdx, squarerax = squarerax * [workp + 20*8];
	zq0 = squarerax;
	squarer01 = squarerdx;
	squarerax = [workp + 20*8];
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * [workp + 21*8];
	zq1 = squarerax;
	squarer11 = squarerdx;
	squarerax = [workp + 20*8];
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * [workp + 22*8];
	zq2 = squarerax;
	squarer21 = squarerdx;
	squarerax = [workp + 20*8];
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * [workp + 23*8];
	zq3 = squarerax;
	squarer31 = squarerdx;
	squarerax = [workp + 20*8];
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * [workp + 24*8];
	zq4 = squarerax;
	squarer41 = squarerdx;
	squarerax = [workp + 21*8];
	squarerdx, squarerax = squarerax * [workp + 21*8];
	cf, zq2 += squarerax;
	_, squarer21 += squarerdx + cf;
	squarerax = [workp + 21*8];
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * [workp + 22*8];
	cf, zq3 += squarerax;
	_, squarer31 += squarerdx + cf;
	squarerax = [workp + 21*8];
	squarerax <<= 1;
	squarerdx, squarerax = squarerax * [workp + 23*8];
	cf, zq4 += squarerax;
	_, squarer41 += squarerdx + cf;
	squarerax = [workp + 21*8];
	squarerax *= 38;
	squarerdx, squarerax = squarerax * [workp + 24*8];
	cf, zq0 += squarerax;
	_, squarer01 += squarerdx + cf;
	squarerax = [workp + 22*8];
	squarerdx, squarerax = squarerax * [workp + 22*8];
	cf, zq4 += squarerax;
	_, squarer41 += squarerdx + cf;
	squarerax = [workp + 22*8];
	squarerax *= 38;
	squarerdx, squarerax = squarerax * [workp + 23*8];
	cf, zq0 += squarerax;
	_, squarer01 += squarerdx + cf;
	squarerax = [workp + 22*8];
	squarerax *= 38;
	squarerdx, squarerax = squarerax * [workp + 24*8];
	cf, zq1 += squarerax;
	_, squarer11 += squarerdx + cf;
	squarerax = [workp + 23*8];
	squarerax *= 19;
	squarerdx, squarerax = squarerax * [workp + 23*8];
	cf, zq1 += squarerax;
	_, squarer11 += squarerdx + cf;
	squarerax = [workp + 23*8];
	squarerax *= 38;
	squarerdx, squarerax = squarerax * [workp + 24*8];
	cf, zq2 += squarerax;
	_, squarer21 += squarerdx + cf;
	squarerax = [workp + 24*8];
	squarerax *= 19;
	squarerdx, squarerax = squarerax * [workp + 24*8];
	cf, zq3 += squarerax;
	_, squarer31 += squarerdx + cf;
	squareredmask = crypto_scalarmult_curve25519_amd64_51_REDMASK51;
	_, _, _, _, _, squarer01 = #x86_SHLD(squarer01, zq0, 13);
	zq0 &= squareredmask;
	_, _, _, _, _, squarer11 = #x86_SHLD(squarer11, zq1, 13);
	zq1 &= squareredmask;
	zq1 += squarer01;
	_, _, _, _, _, squarer21 = #x86_SHLD(squarer21, zq2, 13);
	zq2 &= squareredmask;
	zq2 += squarer11;
	_, _, _, _, _, squarer31 = #x86_SHLD(squarer31, zq3, 13);
	zq3 &= squareredmask;
	zq3 += squarer21;
	_, _, _, _, _, squarer41 = #x86_SHLD(squarer41, zq4, 13);
	zq4 &= squareredmask;
	zq4 += squarer31;
	squarer41 = squarer41 * 19;
	zq0 += squarer41;
	squaret = zq0;
	squaret >>= 51;
	squaret += zq1;
	zq0 &= squareredmask;
	zq1 = squaret;
	squaret >>= 51;
	squaret += zq2;
	zq1 &= squareredmask;
	zq2 = squaret;
	squaret >>= 51;
	squaret += zq3;
	zq2 &= squareredmask;
	zq3 = squaret;
	squaret >>= 51;
	squaret += zq4;
	zq3 &= squareredmask;
	zq4 = squaret;
	squaret >>= 51;
	squaret *= 19;
	zq0 += squaret;
	zq4 &= squareredmask;
	[workp + 20*8] = zq0;
	[workp + 21*8] = zq1;
	[workp + 22*8] = zq2;
	[workp + 23*8] = zq3;
	[workp + 24*8] = zq4;
	mulrax = [workp + 23*8];
	mulrax *= 19;
	mulx319_stack = mulrax;
	mulrdx, mulrax = mulrax * [workp + 2*8];
	zq0 = mulrax;
	mulr01 = mulrdx;
	mulrax = [workp + 24*8];
	mulrax *= 19;
	mulx419_stack = mulrax;
	mulrdx, mulrax = mulrax * [workp + 1*8];
	cf, zq0 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = [workp + 20*8];
	mulrdx, mulrax = mulrax * [workp + 0*8];
	cf, zq0 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = [workp + 20*8];
	mulrdx, mulrax = mulrax * [workp + 1*8];
	zq1 = mulrax;
	mulr11 = mulrdx;
	mulrax = [workp + 20*8];
	mulrdx, mulrax = mulrax * [workp + 2*8];
	zq2 = mulrax;
	mulr21 = mulrdx;
	mulrax = [workp + 20*8];
	mulrdx, mulrax = mulrax * [workp + 3*8];
	zq3 = mulrax;
	mulr31 = mulrdx;
	mulrax = [workp + 20*8];
	mulrdx, mulrax = mulrax * [workp + 4*8];
	zq4 = mulrax;
	mulr41 = mulrdx;
	mulrax = [workp + 21*8];
	mulrdx, mulrax = mulrax * [workp + 0*8];
	cf, zq1 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = [workp + 21*8];
	mulrdx, mulrax = mulrax * [workp + 1*8];
	cf, zq2 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = [workp + 21*8];
	mulrdx, mulrax = mulrax * [workp + 2*8];
	cf, zq3 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulrax = [workp + 21*8];
	mulrdx, mulrax = mulrax * [workp + 3*8];
	cf, zq4 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = [workp + 21*8];
	mulrax *= 19;
	mulrdx, mulrax = mulrax * [workp + 4*8];
	cf, zq0 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = [workp + 22*8];
	mulrdx, mulrax = mulrax * [workp + 0*8];
	cf, zq2 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = [workp + 22*8];
	mulrdx, mulrax = mulrax * [workp + 1*8];
	cf, zq3 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulrax = [workp + 22*8];
	mulrdx, mulrax = mulrax * [workp + 2*8];
	cf, zq4 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = [workp + 22*8];
	mulrax *= 19;
	mulrdx, mulrax = mulrax * [workp + 3*8];
	cf, zq0 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = [workp + 22*8];
	mulrax *= 19;
	mulrdx, mulrax = mulrax * [workp + 4*8];
	cf, zq1 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = [workp + 23*8];
	mulrdx, mulrax = mulrax * [workp + 0*8];
	cf, zq3 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulrax = [workp + 23*8];
	mulrdx, mulrax = mulrax * [workp + 1*8];
	cf, zq4 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = mulx319_stack;
	mulrdx, mulrax = mulrax * [workp + 3*8];
	cf, zq1 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = mulx319_stack;
	mulrdx, mulrax = mulrax * [workp + 4*8];
	cf, zq2 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = [workp + 24*8];
	mulrdx, mulrax = mulrax * [workp + 0*8];
	cf, zq4 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = mulx419_stack;
	mulrdx, mulrax = mulrax * [workp + 2*8];
	cf, zq1 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = mulx419_stack;
	mulrdx, mulrax = mulrax * [workp + 3*8];
	cf, zq2 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = mulx419_stack;
	mulrdx, mulrax = mulrax * [workp + 4*8];
	cf, zq3 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulredmask = crypto_scalarmult_curve25519_amd64_51_REDMASK51;
	_, _, _, _, _, mulr01 = #x86_SHLD(mulr01, zq0, 13);
	zq0 &= mulredmask;
	_, _, _, _, _, mulr11 = #x86_SHLD(mulr11, zq1, 13);
	zq1 &= mulredmask;
	zq1 += mulr01;
	_, _, _, _, _, mulr21 = #x86_SHLD(mulr21, zq2, 13);
	zq2 &= mulredmask;
	zq2 += mulr11;
	_, _, _, _, _, mulr31 = #x86_SHLD(mulr31, zq3, 13);
	zq3 &= mulredmask;
	zq3 += mulr21;
	_, _, _, _, _, mulr41 = #x86_SHLD(mulr41, zq4, 13);
	zq4 &= mulredmask;
	zq4 += mulr31;
	mulr41 = mulr41 * 19;
	zq0 += mulr41;
	mult = zq0;
	mult >>= 51;
	mult += zq1;
	zq1 = mult;
	mult >>= 51;
	zq0 &= mulredmask;
	mult += zq2;
	zq2 = mult;
	mult >>= 51;
	zq1 &= mulredmask;
	mult += zq3;
	zq3 = mult;
	mult >>= 51;
	zq2 &= mulredmask;
	mult += zq4;
	zq4 = mult;
	mult >>= 51;
	zq3 &= mulredmask;
	mult *= 19;
	zq0 += mult;
	zq4 &= mulredmask;
	[workp + 20*8] = zq0;
	[workp + 21*8] = zq1;
	[workp + 22*8] = zq2;
	[workp + 23*8] = zq3;
	[workp + 24*8] = zq4;
	mulrax = t63_stack;
	mulrax *= 19;
	mulx319_stack = mulrax;
	mulrdx, mulrax = mulrax * t72_stack;
	xp0 = mulrax;
	mulr01 = mulrdx;
	mulrax = t64_stack;
	mulrax *= 19;
	mulx419_stack = mulrax;
	mulrdx, mulrax = mulrax * t71_stack;
	cf, xp0 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = t60_stack;
	mulrdx, mulrax = mulrax * t70_stack;
	cf, xp0 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = t60_stack;
	mulrdx, mulrax = mulrax * t71_stack;
	xp1 = mulrax;
	mulr11 = mulrdx;
	mulrax = t60_stack;
	mulrdx, mulrax = mulrax * t72_stack;
	xp2 = mulrax;
	mulr21 = mulrdx;
	mulrax = t60_stack;
	mulrdx, mulrax = mulrax * t73_stack;
	xp3 = mulrax;
	mulr31 = mulrdx;
	mulrax = t60_stack;
	mulrdx, mulrax = mulrax * t74_stack;
	xp4 = mulrax;
	mulr41 = mulrdx;
	mulrax = t61_stack;
	mulrdx, mulrax = mulrax * t70_stack;
	cf, xp1 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = t61_stack;
	mulrdx, mulrax = mulrax * t71_stack;
	cf, xp2 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = t61_stack;
	mulrdx, mulrax = mulrax * t72_stack;
	cf, xp3 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulrax = t61_stack;
	mulrdx, mulrax = mulrax * t73_stack;
	cf, xp4 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = t61_stack;
	mulrax *= 19;
	mulrdx, mulrax = mulrax * t74_stack;
	cf, xp0 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = t62_stack;
	mulrdx, mulrax = mulrax * t70_stack;
	cf, xp2 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = t62_stack;
	mulrdx, mulrax = mulrax * t71_stack;
	cf, xp3 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulrax = t62_stack;
	mulrdx, mulrax = mulrax * t72_stack;
	cf, xp4 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = t62_stack;
	mulrax *= 19;
	mulrdx, mulrax = mulrax * t73_stack;
	cf, xp0 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = t62_stack;
	mulrax *= 19;
	mulrdx, mulrax = mulrax * t74_stack;
	cf, xp1 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = t63_stack;
	mulrdx, mulrax = mulrax * t70_stack;
	cf, xp3 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulrax = t63_stack;
	mulrdx, mulrax = mulrax * t71_stack;
	cf, xp4 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = mulx319_stack;
	mulrdx, mulrax = mulrax * t73_stack;
	cf, xp1 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = mulx319_stack;
	mulrdx, mulrax = mulrax * t74_stack;
	cf, xp2 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = t64_stack;
	mulrdx, mulrax = mulrax * t70_stack;
	cf, xp4 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = mulx419_stack;
	mulrdx, mulrax = mulrax * t72_stack;
	cf, xp1 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = mulx419_stack;
	mulrdx, mulrax = mulrax * t73_stack;
	cf, xp2 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = mulx419_stack;
	mulrdx, mulrax = mulrax * t74_stack;
	cf, xp3 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulredmask = crypto_scalarmult_curve25519_amd64_51_REDMASK51;
	_, _, _, _, _, mulr01 = #x86_SHLD(mulr01, xp0, 13);
	xp0 &= mulredmask;
	_, _, _, _, _, mulr11 = #x86_SHLD(mulr11, xp1, 13);
	xp1 &= mulredmask;
	xp1 += mulr01;
	_, _, _, _, _, mulr21 = #x86_SHLD(mulr21, xp2, 13);
	xp2 &= mulredmask;
	xp2 += mulr11;
	_, _, _, _, _, mulr31 = #x86_SHLD(mulr31, xp3, 13);
	xp3 &= mulredmask;
	xp3 += mulr21;
	_, _, _, _, _, mulr41 = #x86_SHLD(mulr41, xp4, 13);
	xp4 &= mulredmask;
	xp4 += mulr31;
	mulr41 = mulr41 * 19;
	xp0 += mulr41;
	mult = xp0;
	mult >>= 51;
	mult += xp1;
	xp1 = mult;
	mult >>= 51;
	xp0 &= mulredmask;
	mult += xp2;
	xp2 = mult;
	mult >>= 51;
	xp1 &= mulredmask;
	mult += xp3;
	xp3 = mult;
	mult >>= 51;
	xp2 &= mulredmask;
	mult += xp4;
	xp4 = mult;
	mult >>= 51;
	xp3 &= mulredmask;
	mult *= 19;
	xp0 += mult;
	xp4 &= mulredmask;
	[workp + 5*8] = xp0;
	[workp + 6*8] = xp1;
	[workp + 7*8] = xp2;
	[workp + 8*8] = xp3;
	[workp + 9*8] = xp4;
	mul121666rax = t50_stack;
	mul121666rdx, mul121666rax = mul121666rax * crypto_scalarmult_curve25519_amd64_51_121666_213;
	mul121666rax >>= 13;
	zp0 = mul121666rax;
	zp1 = mul121666rdx;
	mul121666rax = t51_stack;
	mul121666rdx, mul121666rax = mul121666rax * crypto_scalarmult_curve25519_amd64_51_121666_213;
	mul121666rax >>= 13;
	zp1 += mul121666rax;
	zp2 = mul121666rdx;
	mul121666rax = t52_stack;
	mul121666rdx, mul121666rax = mul121666rax * crypto_scalarmult_curve25519_amd64_51_121666_213;
	mul121666rax >>= 13;
	zp2 += mul121666rax;
	zp3 = mul121666rdx;
	mul121666rax = t53_stack;
	mul121666rdx, mul121666rax = mul121666rax * crypto_scalarmult_curve25519_amd64_51_121666_213;
	mul121666rax >>= 13;
	zp3 += mul121666rax;
	zp4 = mul121666rdx;
	mul121666rax = t54_stack;
	mul121666rdx, mul121666rax = mul121666rax * crypto_scalarmult_curve25519_amd64_51_121666_213;
	mul121666rax >>= 13;
	zp4 += mul121666rax;
	mul121666rdx *= 19;
	zp0 += mul121666rdx;
	zp0 += t70_stack;
	zp1 += t71_stack;
	zp2 += t72_stack;
	zp3 += t73_stack;
	zp4 += t74_stack;
	[workp + 10*8] = zp0;
	[workp + 11*8] = zp1;
	[workp + 12*8] = zp2;
	[workp + 13*8] = zp3;
	[workp + 14*8] = zp4;
	mulrax = [workp + 13*8];
	mulrax *= 19;
	mulx319_stack = mulrax;
	mulrdx, mulrax = mulrax * t52_stack;
	zp0 = mulrax;
	mulr01 = mulrdx;
	mulrax = [workp + 14*8];
	mulrax *= 19;
	mulx419_stack = mulrax;
	mulrdx, mulrax = mulrax * t51_stack;
	cf, zp0 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = [workp + 10*8];
	mulrdx, mulrax = mulrax * t50_stack;
	cf, zp0 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = [workp + 10*8];
	mulrdx, mulrax = mulrax * t51_stack;
	zp1 = mulrax;
	mulr11 = mulrdx;
	mulrax = [workp + 10*8];
	mulrdx, mulrax = mulrax * t52_stack;
	zp2 = mulrax;
	mulr21 = mulrdx;
	mulrax = [workp + 10*8];
	mulrdx, mulrax = mulrax * t53_stack;
	zp3 = mulrax;
	mulr31 = mulrdx;
	mulrax = [workp + 10*8];
	mulrdx, mulrax = mulrax * t54_stack;
	zp4 = mulrax;
	mulr41 = mulrdx;
	mulrax = [workp + 11*8];
	mulrdx, mulrax = mulrax * t50_stack;
	cf, zp1 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = [workp + 11*8];
	mulrdx, mulrax = mulrax * t51_stack;
	cf, zp2 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = [workp + 11*8];
	mulrdx, mulrax = mulrax * t52_stack;
	cf, zp3 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulrax = [workp + 11*8];
	mulrdx, mulrax = mulrax * t53_stack;
	cf, zp4 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = [workp + 11*8];
	mulrax *= 19;
	mulrdx, mulrax = mulrax * t54_stack;
	cf, zp0 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = [workp + 12*8];
	mulrdx, mulrax = mulrax * t50_stack;
	cf, zp2 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = [workp + 12*8];
	mulrdx, mulrax = mulrax * t51_stack;
	cf, zp3 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulrax = [workp + 12*8];
	mulrdx, mulrax = mulrax * t52_stack;
	cf, zp4 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = [workp + 12*8];
	mulrax *= 19;
	mulrdx, mulrax = mulrax * t53_stack;
	cf, zp0 += mulrax;
	_, mulr01 += mulrdx + cf;
	mulrax = [workp + 12*8];
	mulrax *= 19;
	mulrdx, mulrax = mulrax * t54_stack;
	cf, zp1 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = [workp + 13*8];
	mulrdx, mulrax = mulrax * t50_stack;
	cf, zp3 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulrax = [workp + 13*8];
	mulrdx, mulrax = mulrax * t51_stack;
	cf, zp4 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = mulx319_stack;
	mulrdx, mulrax = mulrax * t53_stack;
	cf, zp1 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = mulx319_stack;
	mulrdx, mulrax = mulrax * t54_stack;
	cf, zp2 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = [workp + 14*8];
	mulrdx, mulrax = mulrax * t50_stack;
	cf, zp4 += mulrax;
	_, mulr41 += mulrdx + cf;
	mulrax = mulx419_stack;
	mulrdx, mulrax = mulrax * t52_stack;
	cf, zp1 += mulrax;
	_, mulr11 += mulrdx + cf;
	mulrax = mulx419_stack;
	mulrdx, mulrax = mulrax * t53_stack;
	cf, zp2 += mulrax;
	_, mulr21 += mulrdx + cf;
	mulrax = mulx419_stack;
	mulrdx, mulrax = mulrax * t54_stack;
	cf, zp3 += mulrax;
	_, mulr31 += mulrdx + cf;
	mulredmask = crypto_scalarmult_curve25519_amd64_51_REDMASK51;
	_, _, _, _, _, mulr01 = #x86_SHLD(mulr01, zp0, 13);
	zp0 &= mulredmask;
	_, _, _, _, _, mulr11 = #x86_SHLD(mulr11, zp1, 13);
	zp1 &= mulredmask;
	zp1 += mulr01;
	_, _, _, _, _, mulr21 = #x86_SHLD(mulr21, zp2, 13);
	zp2 &= mulredmask;
	zp2 += mulr11;
	_, _, _, _, _, mulr31 = #x86_SHLD(mulr31, zp3, 13);
	zp3 &= mulredmask;
	zp3 += mulr21;
	_, _, _, _, _, mulr41 = #x86_SHLD(mulr41, zp4, 13);
	zp4 &= mulredmask;
	zp4 += mulr31;
	mulr41 = mulr41 * 19;
	zp0 += mulr41;
	mult = zp0;
	mult >>= 51;
	mult += zp1;
	zp1 = mult;
	mult >>= 51;
	zp0 &= mulredmask;
	mult += zp2;
	zp2 = mult;
	mult >>= 51;
	zp1 &= mulredmask;
	mult += zp3;
	zp3 = mult;
	mult >>= 51;
	zp2 &= mulredmask;
	mult += zp4;
	zp4 = mult;
	mult >>= 51;
	zp3 &= mulredmask;
	mult *= 19;
	zp0 += mult;
	zp4 &= mulredmask;
	[workp + 10*8] = zp0;
	[workp + 11*8] = zp1;
	[workp + 12*8] = zp2;
	[workp + 13*8] = zp3;
	[workp + 14*8] = zp4;
	return;
}


