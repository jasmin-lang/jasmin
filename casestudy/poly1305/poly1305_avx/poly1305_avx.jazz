// little endian for loads and stores
fn load2(reg u64 p) -> reg u64[2]
{
  reg u64[2] x;
  x[0] = [p + 0];
  x[1] = [p + 8];
  return x;
}



fn load_add(reg u64[3] h, reg u64 in) -> reg u64[3]
{
  reg bool cf;
  cf, h[0] += [in + 0];
  cf, h[1] += [in + 8] + cf;
   _, h[2] +=        1 + cf;
  return h;
}



fn load_last_add(reg u64[3] h, reg u64 in, reg u64 len) -> reg u64[3]
{
  reg bool cf;
  reg   u64    j;
  stack u64[2] s;
  reg   u8     c;

  s[0] = 0;
  s[1] = 0;

  j = 0;
  while(j < len)
  { c = (u8)[in + j];
    s[u8 (int)j] = c;
    j += 1;
  }

  s[u8 (int)j] = 0x1;

  cf, h[0] += s[0];
  cf, h[1] += s[1] + cf;
   _, h[2] +=    0 + cf;
 
  return h;
} 



fn store2(reg u64 p, reg u64[2] x)
{
  [p + 0] = x[0];
  [p + 8] = x[1];
}



fn clamp(reg u64 k) -> reg u64[3]
{
  reg u64[3] r;
  r[0] = [k + 0];
  r[1] = [k + 8];
  r[0] &= 0x0ffffffc0fffffff;
  r[1] &= 0x0ffffffc0ffffffc;
  r[2] = r[1];
  r[2] >>= 2;
  r[2] += r[1];
  return r;
}



// h += s
fn add2(reg u64[2] h, reg u64[2] s) -> reg u64[2]
{
  reg bool cf;
  cf, h[0] += s[0];
   _, h[1] += s[1] + cf;
  return h;
}



fn mulmod(reg u64[3] h, reg u64[3] r) -> reg u64[3]
{
  reg bool cf;
  reg u64 t0 t1 t2;
  reg u64 rax rdx;

  t2 = r[2];
  t2 *= h[2];     // (* t2 = h[2]*r[2] *)
  h[2] *= r[0];   // (* h[2] = h[2]*r[0] *)

  rax = r[0];
  rdx, rax = rax * h[0];
  t0 = rax;      // (* t0 = h[0]*r[0] *)
  t1 = rdx;      // (* t1 = mulhi h[0] r[0] *)

  rax = r[0];
  rdx, rax = rax * h[1];

  cf, t1 += rax;           // (* t1 = h[1]*r[0] + mulhi h[0] r[0]*)
  _ , h[2] += rdx + cf;    // (* h[2] = h[2]*r[0] + mulhi h[1] r[0] + CF *)

  rax = r[2];
  rdx, rax = rax * h[1];
  h[1] = rdx;
  h[1] += t2;    // (* h[1] = h[2]*r[2] + mulhi h[1] r[2] *)
  t2 = rax;      // (* t2 = h[1]*r[2] *)

  rax = r[1];
  rdx, rax = rax * h[0];

  cf, t0 += t2;            // (* t0 = h[0]*r[0] + h[1]*r[2] *)
  cf, t1 += rax + cf;      // (* t1 = h[0]*r[1] + t1 + CF *)
  _ , h[2] += rdx + cf;    // (* h[2] = mulhi h[0] r[1] + h[2] + CF *)

  h[0] = 0xfffffffffffffffc;
    t2 = h[2]; 
    t2 >>= 2;
  h[0] &= h[2];

  h[0] += t2;
  h[2] &= 0x03;

  cf, h[0] += t0;
  cf, h[1] += t1 + cf;
  _ , h[2] +=  0 + cf;

  return h;
}



fn freeze(reg u64[3] h) -> reg u64[2]
{
  reg bool cf;
  reg u64[2] g;
  reg u64 g2;
  reg u64 mask;

  g[0] = h[0];
  g[1] = h[1];
  g2 = h[2];

  //                  <= 6 then g[2] can be at most 7 (111b)
  // if h[2] value is <= 4 then g[2] can be at most 5 (101b)
  cf, g[0] += 5;
  cf, g[1] += 0 + cf;
   _, g2 += 0 + cf;

  // which means that by shifting right by 2 we are left with only 1 bit set
  g2 >>= 2;

  // and if this bit is set g[2]: mask will be 2**64-1 (all bits are set) otherwise
  // the mask will be zero
  mask = -g2;

  g[0] ^= h[0];
  g[1] ^= h[1];

  g[0] &= mask;
  g[1] &= mask;

  // if bit == 1 then h[0..1] ^= (g[0..1] ^ h[0..1])
  // else             h[0..1] ^= 0
  g[0] ^= h[0];
  g[1] ^= h[1];

  // at this point we only need the first 128 bits
  return g;
}



fn poly1305_ref3_setup(reg u64 k) -> reg u64[3], reg u64[3], reg u64
{
  inline int i;
  reg u64[3] h;
  reg u64[3] r;
  reg u64 len;

  for i=0 to 3 { h[i] = 0; }
  r = clamp(k);
  k += 16;
  return h, r, k;
}



fn poly1305_ref3_update(reg u64 in, reg u64 inlen, reg u64[3] h, reg u64[3] r) -> reg u64, reg u64, reg u64[3]
{
  reg bool cf;
  reg u64[2] m;

  while(inlen >= 16)
  {
    h = load_add(h, in);
    h = mulmod(h, r);
    in += 16;
    inlen -= 16;
  }

  return in, inlen, h;
}



fn poly1305_ref3_last(reg u64 out, reg u64 in, reg u64 inlen, reg u64 k, reg u64[3] h, reg u64[3] r)
{
  reg u64[2] m, s;
  reg u64[2] h2;

  if(inlen > 0)
  { h = load_last_add(h, in, inlen);
    h = mulmod(h, r);
  }

  h2 = freeze(h);
  s = load2(k);
  h2 = add2(h2, s);

  store2(out, h2);
}



fn poly1305_ref3_local(reg u64 out, reg u64 in, reg u64 inlen, reg u64 k)
{
  reg u64[3] h;
  reg u64[3] r;
  reg u64 len;

  h, r, k = poly1305_ref3_setup(k);
  len = inlen;
  in, len, h = poly1305_ref3_update(in, len, h, r);
  poly1305_ref3_last(out, in, len, k, h, r);
}




u64 zero_u64 = 0;
u64 five_u64 = 5;
u64 mask26_u64 = 0x3ffffff;
u64 bit25_u64 = 0x1000000;




fn times_5(stack u128[5] r12) -> stack u128[4]
{
  inline int i;
  stack u128[4] r12x5;
  reg u128 t, five;

  five = #VPBROADCAST_2u64(five_u64);
  for i=0 to 4
  { t = #VPMULU(five, r12[1+i]);
    r12x5[i] = t; }
  return r12x5;
}


fn broadcast_r2(stack u128[5] r12, stack u128[4] r12x5) -> stack u128[5], stack u128[4]
{
  inline int i;
  stack u128[5] r22;
  stack u128[4] r22x5;
  reg   u128[5] t;

  for i=0 to 5
  { t[i] = #VPBROADCAST_2u64(r12[u64 2*i]);
    r22[i] = t[i]; }

  for i=0 to 4
  { t[i] = #VPBROADCAST_2u64(r12x5[u64 2*i]);
    r22x5[i] = t[i]; }

  return r22, r22x5;
}


fn broadcast_r4(reg u64[3] r4) -> stack u128[5], stack u128[4]
{
  inline int i mask26;
  stack u128[5] r44;
  stack u128[4] r44x5;
  reg u64[5] t;
  reg u64 h, l;


  //r44 = unpack(r44, r4, 0);
  
  mask26 = 0x3ffffff;
  l =  r4[0];
  l &= mask26;
  r44[u64  0 + 0] = l;
  l =  r4[0];
  l >>= 26;
  l &= mask26;
  r44[u64  0 + 2] = l;
  l =  r4[0];
  l = #SHRD(l,  r4[1], 52);
  h = l;
  l &= mask26;
  r44[u64  0 + 4] = l;
  l = h;
  l >>= 26;
  l &= mask26;
  r44[u64  0 + 6] = l;
  l =  r4[1];
  l = #SHRD(l,  r4[2], 40);
  r44[u64  0 + 8] = l;

  for i=0 to 5
  { t[i] = r44[u64 2*i]; r44[u64 1+2*i] = t[i]; }

  r44x5 = times_5(r44);

  return r44, r44x5;
}



fn poly1305_avx_setup(reg u64[3] r) ->
  stack u128[5], stack u128[4],
  stack u128[5], stack u128[4],
  stack u128[5], stack u128[4]
{
  inline int i mask26;
  stack u128[5] r44 r22 r12;
  stack u128[4] r44x5 r22x5 r12x5;
  reg u128 t;
  reg u64[3] rt;
  reg u64 h, l;


  // rt = r; store rt
  for i=0 to 2 { rt[i] = r[i]; } rt[2] = 0;
  //r12 = unpack(r12, rt, 1); // r^1
  
  mask26 = 0x3ffffff;
  l =  rt[0];
  l &= mask26;
  r12[u64  1 + 0] = l;
  l =  rt[0];
  l >>= 26;
  l &= mask26;
  r12[u64  1 + 2] = l;
  l =  rt[0];
  l = #SHRD(l,  rt[1], 52);
  h = l;
  l &= mask26;
  r12[u64  1 + 4] = l;
  l = h;
  l >>= 26;
  l &= mask26;
  r12[u64  1 + 6] = l;
  l =  rt[1];
  l = #SHRD(l,  rt[2], 40);
  r12[u64  1 + 8] = l;

  rt = mulmod(rt, r); // r^2
  //r12 = unpack(r12, rt, 0);
  
  mask26 = 0x3ffffff;
  l =  rt[0];
  l &= mask26;
  r12[u64  0 + 0] = l;
  l =  rt[0];
  l >>= 26;
  l &= mask26;
  r12[u64  0 + 2] = l;
  l =  rt[0];
  l = #SHRD(l,  rt[1], 52);
  h = l;
  l &= mask26;
  r12[u64  0 + 4] = l;
  l = h;
  l >>= 26;
  l &= mask26;
  r12[u64  0 + 6] = l;
  l =  rt[1];
  l = #SHRD(l,  rt[2], 40);
  r12[u64  0 + 8] = l;

  r12x5 = times_5(r12);
  r22, r22x5 = broadcast_r2(r12, r12x5);

  rt = mulmod(rt, r); // r^3
  rt = mulmod(rt, r); // r^4

  r44, r44x5 = broadcast_r4(rt);

  return r44, r44x5, r22, r22x5, r12, r12x5;
}



fn load_avx(reg u64 in, reg u128 mask26, stack u128 s_bit25) -> reg u128[5], reg u64
{
  reg u128 t, m0, m1;
  reg u128[5] m;

  t = (u128)[in +  0];
  m[3] = (u128)[in + 16];
  m[0] = #VPUNPCKL_2u64(t, m[3]);
  m[3] = #VPUNPCKH_2u64(t, m[3]);
  m[1] = m[0];
  m[2] = m[0];
  m[4] = m[3];
  m[0] &= mask26;
  m[1] >>2u64= 26;
  m[1] &= mask26;
  m[2] >>2u64= 52;
  t = m[3] <<2u64 12;
  m[2] |= t;
  m[2] &= mask26;
  m[3] >>2u64= 14;
  m[3] &= mask26;
  m[4] >>2u64= 40;
  m[4] |= s_bit25;

  in += 32;

  return m, in;
}



fn pack_avx(reg u128[5] h) -> reg u64[3]
{
  reg bool cf;
  inline int i;
  reg u128[3] t;
  reg u128 t0;
  reg u128[2] u;
  reg u64[3] d r;
  reg u64 c cx4;

  // we start by saying that t0 will be equal to:
  //   { a1+b1*2^26, a0+b0*2^26 }
  // and for simplicity we can just write
  //   { ab(1), ab(0) }
  t[0] = h[1] <<2u64 26;
  t[0] +2u64= h[0];

  // and t1 will be equal to:
  //   { c1+d1*2^26, c0+d0*2^26 }
  // and for simplicity we can just write
  //   { cd(1), cd(0) }
  t[1] = h[3] <<2u64 26;
  t[1] +2u64= h[2];

  // and t2 will be equal to:
  //   { e1, e(0+1) }
  t[2] = #VPSRLDQ_128(h[4], 8);
  t[2] +2u64= h[4];


  // u0 = { cd0 , ab0 }
  // u1 = { cd1 , ab1 }
  u[0] = #VPUNPCKL_2u64(t[0], t[1]);
  u[1] = #VPUNPCKH_2u64(t[0], t[1]);

  // t0 = { cd0+cd1 , ab0+ab1 } 
  t[0] = u[0] +2u64 u[1];


  // extract all values into u64 registers
  d[0] = #VPEXTR_64(t[0], 0); // TODO check how many bits
  d[1] = #VPEXTR_64(t[0], 1); // same comment
  d[2] = #VPEXTR_64(t[2], 0); // same comment

  // at this point we have that 
  //   R = r0*2^0 + r1^2^52 + r2*2^104
  // and we want it to be
  //   R = r0*2^0 + r1*2^64 + r2*2^128

  r[0] = d[1];
  r[0] <<= 52; // 12 bits from d[1]

  r[1] = d[1];
  r[1] >>= 12; // 52 bits from d[1] (only ~43 should be set)

  r[2] = d[2];
  r[2] >>= 24; // 128 - 104

  d[2] <<= 40; // 64 - (128 - 104)

  cf, r[0] += d[0];
  cf, r[1] += d[2] + cf;
   _, r[2] += 0 + cf;

  // reduce (check comments in mulmod function of ref3 implementation)
  c = r[2];
  cx4 = r[2];
  r[2] &= 3; // clear the remaining bits
  c >>= 2; // (r[2]>>2)
  cx4 &= -4; // clear first 2 bits: (r[2]>>2)<<2
  c += cx4; 

  cf, r[0] += c;
  cf, r[1] += 0 + cf;
   _, r[2] += 0 + cf;

  return r; 
}



fn carry_reduce_avx(reg u128[5] x, reg u128 mask26) -> reg u128[5]
{
  reg u128[2] z;
  reg u128 t;

  z[0] = x[0] >>2u64 26;
  z[1] = x[3] >>2u64 26;

  x[0] &= mask26;
  x[3] &= mask26;

  x[1] +2u64= z[0];
  x[4] +2u64= z[1];

  z[0] = x[1] >>2u64 26;
  z[1] = x[4] >>2u64 26;

  t = z[1] <<2u64 2;
  z[1] +2u64= t;

  x[1] &= mask26;
  x[4] &= mask26;
  x[2] +2u64= z[0];
  x[0] +2u64= z[1];

  z[0] = x[2] >>2u64 26;
  z[1] = x[0] >>2u64 26;
  x[2] &= mask26;
  x[0] &= mask26;
  x[3] +2u64= z[0];
  x[1] +2u64= z[1];

  z[0] = x[3] >>2u64 26;
  x[3] &= mask26;
  x[4] +2u64= z[0];

  return x;
}



// TODO: write some comments about this function
fn mulmod_avx(
  reg u128[5] h,
  stack u128[5] s_r,
  stack u128[4] s_rx5,
  stack u128 s_mask26,
  stack u128 s_bit25
) -> reg u128[5]
{
  reg u128[5] t;
  reg u128[4] u;
  reg u128 r0, r1, r4x5, r2, r3x5, r3, r2x5;
  reg u128 mask26;

  // pre fetching first 3 r's
  r0 = s_r[0];
  r1 = s_r[1];
  r4x5 = s_rx5[4-1];

  // t0 = h0*r0
  // t1 = h1*r0
  // t2 = h2*r0
  // t3 = h3*r0
  // t4 = h4*r0
  t[0] = #VPMULU(h[0], r0);
  t[1] = #VPMULU(h[1], r0);
  t[2] = #VPMULU(h[2], r0);
  t[3] = #VPMULU(h[3], r0);
  t[4] = #VPMULU(h[4], r0);

  // t1 += h0*r1
  // t2 += h1*r1
  // t3 += h2*r1
  // t4 += h3*r1
  u[0] = #VPMULU(h[0], r1);
  u[1] = #VPMULU(h[1], r1);
  u[2] = #VPMULU(h[2], r1);
  u[3] = #VPMULU(h[3], r1);

  // prefetch r2
  r2 = s_r[2];

  t[1] +2u64= u[0];
  t[2] +2u64= u[1];
  t[3] +2u64= u[2];
  t[4] +2u64= u[3];

  // t0 += h1*r4*5
  // t1 += h2*r4*5
  // t2 += h3*r4*5
  // t3 += h4*r4*5

  u[0] = #VPMULU(h[1], r4x5);
  u[1] = #VPMULU(h[2], r4x5);
  u[2] = #VPMULU(h[3], r4x5);
  u[3] = #VPMULU(h[4], r4x5);

  // prefetch r3*5
  r3x5 = s_rx5[3-1];

  t[0] +2u64= u[0];
  t[1] +2u64= u[1];
  t[2] +2u64= u[2];
  t[3] +2u64= u[3];

  // t2 += h0*r2
  // t3 += h1*r2
  // t4 += h2*r2

  u[0] = #VPMULU(h[0], r2);
  u[1] = #VPMULU(h[1], r2);
  u[2] = #VPMULU(h[2], r2);

  // prefetch r3
  r3 = s_r[3];

  t[2] +2u64= u[0];
  t[3] +2u64= u[1];
  t[4] +2u64= u[2];

  // t0 += h2*r3*5
  // t1 += h3*r3*5
  // t2 += h4*r3*5

  u[0] = #VPMULU(h[2], r3x5); // h2 dead
  u[1] = #VPMULU(h[3], r3x5);
  h[2] = #VPMULU(h[4], r3x5);

  // prefetch r2*5
  r2x5 = s_rx5[2-1];

  t[0] +2u64= u[0];
  t[1] +2u64= u[1];
  h[2] +2u64= t[2]; // t2 dead // h[2] contains final h2

  // t3 += h0*r3
  // t4 += h1*r3

  u[0] = #VPMULU(h[0], r3);
  u[1] = #VPMULU(h[1], r3); // h1 dead

  t[3] +2u64= u[0];
  t[4] +2u64= u[1];

  // t0 += h3*r2*5
  // t1 += h4*r2*5

  u[0] = #VPMULU(h[3], r2x5); // h3 dead
  h[1] = #VPMULU(h[4], r2x5);

  t[0] +2u64= u[0];
  h[1] +2u64= t[1]; // t1 dead // h[1] contains final h1

  // t0 += h4*r1*5
  // t4 += h0*r4

  u[0] = #VPMULU(h[4], s_rx5[1-1]); // h4 dead
  u[1] = #VPMULU(h[0], s_r[4]); // h0 dead

  h[0] = t[0] +2u64 u[0];
  h[3] = t[3];
  h[4] = t[4] +2u64 u[1];

  return h;
}



// H*R^4 + M0*R^2 + M1
fn mainloop_avx_v0(
  reg u128[5] h,
  reg u64 in,
  stack u128[5] s_r44,
  stack u128[4] s_r44x5,
  stack u128[5] s_r22,
  stack u128[4] s_r22x5,
  stack u128 s_mask26,
  stack u128 s_bit25
) -> reg u128[5],
     reg u64
{
  inline int i;
  reg u128 mask26;
  stack u128[5] s_h;
  reg u128[5] m;

  h = mulmod_avx(h, s_r44, s_r44x5, s_mask26, s_bit25);
  s_h = h;

  mask26 = s_mask26;
  h, in = load_avx(in, mask26, s_bit25);
  h = mulmod_avx(h, s_r22, s_r22x5, s_mask26, s_bit25);

  for i=0 to 5
  { h[i] +2u64= s_h[i]; }

  mask26 = s_mask26;
  m, in = load_avx(in, mask26, s_bit25);

  for i=0 to 5
  { h[i] +2u64= m[i]; }

  h = carry_reduce_avx(h, mask26);

  return h,  in;
}



// H*R^4 + M0*R^2 + M1
fn mainloop_avx_v1(
  reg u128[5] h,
  reg u64 in,
  stack u128[5] s_r44,
  stack u128[4] s_r44x5,
  stack u128[5] s_r22,
  stack u128[4] s_r22x5,
  stack u128 s_mask26,
  stack u128 s_bit25
) -> reg u128[5],
     reg u64
{
  inline int i;
  reg u128 mask26;
  stack u128[5] s_h;
  reg u128[5] m;
  reg u128[5] t;
  reg u128[4] u;
  reg u128 r0, r1, r4x5, r2, r3x5, r3, r2x5;
  reg u128 mask26;
  reg u128 m0, m1, mt;

  //h = mulmod_avx(h, s_r44, s_r44x5, s_mask26, s_bit25);

  // pre fetching first 3 r's
  r0 = s_r44[0];
  r1 = s_r44[1];
  r4x5 = s_r44x5[4-1];

  // t0 = h0*r0
  // t1 = h1*r0
  // t2 = h2*r0
  // t3 = h3*r0
  // t4 = h4*r0
  //
  // t1 += h0*r1
  // t2 += h1*r1
  // t3 += h2*r1
  // t4 += h3*r1
  t[0] = #VPMULU(h[0], r0);
  u[0] = #VPMULU(h[0], r1);
  t[1] = #VPMULU(h[1], r0);
  u[1] = #VPMULU(h[1], r1);
  t[2] = #VPMULU(h[2], r0);
  u[2] = #VPMULU(h[2], r1);
  t[3] = #VPMULU(h[3], r0); t[1] +2u64= u[0];
  u[3] = #VPMULU(h[3], r1); t[2] +2u64= u[1];
  t[4] = #VPMULU(h[4], r0); t[3] +2u64= u[2];
                                t[4] +2u64= u[3];

  // t0 += h1*r4*5
  // t1 += h2*r4*5
  // t2 += h3*r4*5
  // t3 += h4*r4*5

  u[0] = #VPMULU(h[1], r4x5); m0 = (u128)[in + 0];
  u[1] = #VPMULU(h[2], r4x5); r2 = s_r44[2];
  u[2] = #VPMULU(h[3], r4x5);
  u[3] = #VPMULU(h[4], r4x5);

  t[0] +2u64= u[0]; m1 = (u128)[in + 16];
  t[1] +2u64= u[1];
  t[2] +2u64= u[2];
  t[3] +2u64= u[3];

  // t2 += h0*r2
  // t3 += h1*r2
  // t4 += h2*r2

  u[0] = #VPMULU(h[0], r2); m[0] = #VPUNPCKL_2u64(m0, m1);
  u[1] = #VPMULU(h[1], r2); m[3] = #VPUNPCKH_2u64(m0, m1);
  u[2] = #VPMULU(h[2], r2);


  t[2] +2u64= u[0]; r3x5 = s_r44x5[3-1];
  t[3] +2u64= u[1];
  t[4] +2u64= u[2];

  // t0 += h2*r3*5
  // t1 += h3*r3*5
  // t2 += h4*r3*5

  u[0] = #VPMULU(h[2], r3x5); 
  u[1] = #VPMULU(h[3], r3x5); m[1] = m[0]; // h2 dead
  h[2] = #VPMULU(h[4], r3x5); m[1] >>2u64= 26;
                                  m[1] &= s_mask26;

  // prefetch r3
  r3 = s_r44[3];

  t[0] +2u64= u[0];
  t[1] +2u64= u[1];
  h[2] +2u64= t[2]; // t2 dead // h[2] contains final h2

  // t3 += h0*r3
  // t4 += h1*r3

  u[0] = #VPMULU(h[0], r3); m[4] = m[3]; s_h[2] = h[2];
  u[1] = #VPMULU(h[1], r3); m[4] >>2u64= 40; // h1 dead
                                m[4] |= s_bit25;
  // prefetch r2*5
  r2x5 = s_r44x5[2-1];

  t[3] +2u64= u[0];
  t[4] +2u64= u[1];

  // t0 += h3*r2*5
  // t1 += h4*r2*5

  u[0] = #VPMULU(h[3], r2x5); m[2] = m[0]; s_h[3] = t[3]; // h3 dead
  h[1] = #VPMULU(h[4], r2x5); m[2] >>2u64= 52;
                                  

  t[0] +2u64= u[0];
  h[1] +2u64= t[1]; // t1 dead // h[1] contains final h1

  // t0 += h4*r1*5
  // t4 += h0*r4

  u[0] = #VPMULU(h[4], s_r44x5[1-1]); mt = m[3] <<2u64 12; s_h[1] = h[1]; // h4 dead
  u[1] = #VPMULU(h[0], s_r44[4]); m[2] |= mt; mask26 = s_mask26; // h0 dead

  h[0] = t[0] +2u64 u[0];
  h[4] = t[4] +2u64 u[1];

  s_h[0] = h[0];
  s_h[4] = h[4];

  //////////////////////////////////////////////////////////////////////////////
  //h, in = load_avx(in, mask26, s_bit25);

  m[0] &= mask26;
  m[2] &= mask26;
  m[3] >>2u64= 14;
  m[3] &= mask26;

  //////////////////////////////////////////////////////////////////////////////
  //h = mulmod_avx(h, s_r22, s_r22x5, s_mask26, s_bit25);

  // pre fetching first 3 r's
  r0 = s_r22[0];
  r1 = s_r22[1];
  r4x5 = s_r22x5[4-1];

  // t0 = h0*r0
  // t1 = h1*r0
  // t2 = h2*r0
  // t3 = h3*r0
  // t4 = h4*r0
  //
  // t1 += h0*r1
  // t2 += h1*r1
  // t3 += h2*r1
  // t4 += h3*r1
  t[0] = #VPMULU(m[0], r0);
  u[0] = #VPMULU(m[0], r1);
  t[1] = #VPMULU(m[1], r0);
  u[1] = #VPMULU(m[1], r1);
  t[2] = #VPMULU(m[2], r0);
  u[2] = #VPMULU(m[2], r1); t[0] +2u64= s_h[0];
  t[3] = #VPMULU(m[3], r0); t[1] +2u64= s_h[1]; t[1] +2u64= u[0];
  u[3] = #VPMULU(m[3], r1); t[2] +2u64= s_h[2]; t[2] +2u64= u[1];
  t[4] = #VPMULU(m[4], r0); t[3] +2u64= s_h[3]; t[3] +2u64= u[2];
                                t[4] +2u64= s_h[4]; t[4] +2u64= u[3];

  // t0 += h1*r4*5
  // t1 += h2*r4*5
  // t2 += h3*r4*5
  // t3 += h4*r4*5

  u[0] = #VPMULU(m[1], r4x5); m0 = (u128)[in + 32];
  u[1] = #VPMULU(m[2], r4x5); r2 = s_r22[2];
  u[2] = #VPMULU(m[3], r4x5);
  u[3] = #VPMULU(m[4], r4x5);

  t[0] +2u64= u[0]; m1 = (u128)[in + 48];
  t[1] +2u64= u[1];
  t[2] +2u64= u[2];
  t[3] +2u64= u[3];

  // t2 += h0*r2
  // t3 += h1*r2
  // t4 += h2*r2

  u[0] = #VPMULU(m[0], r2); h[0] = #VPUNPCKL_2u64(m0, m1);
  u[1] = #VPMULU(m[1], r2); h[3] = #VPUNPCKH_2u64(m0, m1);
  u[2] = #VPMULU(m[2], r2);


  t[2] +2u64= u[0]; r3x5 = s_r22x5[3-1];
  t[3] +2u64= u[1];
  t[4] +2u64= u[2];

  // t0 += h2*r3*5
  // t1 += h3*r3*5
  // t2 += h4*r3*5

  u[0] = #VPMULU(m[2], r3x5); // h2 dead
  u[1] = #VPMULU(m[3], r3x5); h[1] = h[0];
  m[2] = #VPMULU(m[4], r3x5); h[1] >>2u64= 26;
                                  h[1] &= s_mask26;

  // prefetch r3
  r3 = s_r22[3];

  t[0] +2u64= u[0];
  t[1] +2u64= u[1];
  m[2] +2u64= t[2]; // t2 dead // h[2] contains final h2

  // t3 += h0*r3
  // t4 += h1*r3

  u[0] = #VPMULU(m[0], r3); h[4] = h[3];
  u[1] = #VPMULU(m[1], r3); h[4] >>2u64= 40; // h1 dead
                                h[4] |= s_bit25;

  // prefetch r2*5
  r2x5 = s_r22x5[2-1];

  t[3] +2u64= u[0];
  t[4] +2u64= u[1];

  // t0 += h3*r2*5
  // t1 += h4*r2*5

  u[0] = #VPMULU(m[3], r2x5); h[2] = h[0]; // h3 dead
  m[1] = #VPMULU(m[4], r2x5); h[2] >>2u64= 52;

  t[0] +2u64= u[0];
  m[1] +2u64= t[1]; // t1 dead // h[1] contains final h1

  // t0 += h4*r1*5
  // t4 += h0*r4

  u[0] = #VPMULU(m[4], s_r22x5[1-1]); mt = h[3] <<2u64 12; // h4 dead
  u[1] = #VPMULU(m[0], s_r22[4]); h[2] |= mt; mask26 = s_mask26; // h0 dead

  m[0] = t[0] +2u64 u[0];
  m[3] = t[3];
  m[4] = t[4] +2u64 u[1];

  //////////////////////////////////////////////////////////////////////////////
  //m, in = load_avx(in, mask26, s_bit25);

  h[0] &= mask26;  h[0] +2u64= m[0];
  h[2] &= mask26;  h[2] +2u64= m[2];
  h[3] >>2u64= 14; 
  h[3] &= mask26;  h[3] +2u64= m[3];
  in += 64;

  h[1] +2u64= m[1];
  h[4] +2u64= m[4];
  
  h = carry_reduce_avx(h, mask26);

  return h,  in;
}



fn final_avx_v0(
  reg u128[5] h,
  stack u128[5] s_r,
  stack u128[4] s_rx5,
  stack u128 s_mask26,
  stack u128 s_bit25
) -> reg u128[5]
{
  reg u128 mask26;

  h = mulmod_avx(h, s_r, s_rx5, s_mask26, s_bit25);
  mask26 = s_mask26;
  h = carry_reduce_avx(h, mask26);

  return h;
}



fn poly1305_avx_update(
  reg u64 in, reg u64 len,
  stack u128[5] r44,  
  stack u128[4] r44x5,
  stack u128[5] r22,  
  stack u128[4] r22x5,
  stack u128[5] r12,
  stack u128[4] r12x5
) -> reg u64, reg u64, reg u64[3]
{
  inline int i;
  stack u128 s_mask26, s_bit25;
  reg u128[5] h m;
  reg u128 mask26 t;
  reg u64[3] h64;

  for i=0 to 5
  { h[i] = #VPBROADCAST_2u64(zero_u64); } // zero out me better 
  t = #VPBROADCAST_2u64(mask26_u64); s_mask26 = t; mask26 = t;
  t = #VPBROADCAST_2u64(bit25_u64); s_bit25 = t;

  while(len >= 64)
  { 
    h, in = mainloop_avx_v1(h, in, r44, r44x5, r22, r22x5, s_mask26, s_bit25);
    len -= 64;
  }

  h = final_avx_v0(h, r12, r12x5, s_mask26, s_bit25);
  h64 = pack_avx(h);

  return in, len, h64;
}


fn poly1305_avx_wrapper(reg u64 out, reg u64 in, reg u64 inlen, reg u64 k)
{
  reg u64[3] h;
  reg u64[3] r;
  reg u64 len;
  stack u128[5] r44 r22 r12;
  stack u128[4] r44x5 r22x5 r12x5;

  len = inlen;

  h, r, k = poly1305_ref3_setup(k);
  r44, r44x5, r22, r22x5, r12, r12x5 = poly1305_avx_setup(r);

  in, len, h = poly1305_avx_update(in, len, r44, r44x5, r22, r22x5, r12, r12x5);
  in, len, h = poly1305_ref3_update(in, len, h, r);

  poly1305_ref3_last(out, in, len, k, h, r);
}



export fn poly1305_avx(reg u64 out, reg u64 in, reg u64 inlen, reg u64 k)
{
  if(inlen < 1024)
  { poly1305_ref3_local(out, in, inlen, k); }
  else
  { poly1305_avx_wrapper(out, in, inlen, k); }
}
